{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE4WC2_4wygJ"
      },
      "source": [
        "# Assignment 1\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: Sexism Detection, Multi-class Classification, RNNs, Transformers, Huggingface\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL69zGpmx01k"
      },
      "source": [
        "# Contact\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "- Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "- Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "- Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55jnW-xKxi-2"
      },
      "source": [
        "# Introduction\n",
        "You are asked to address the [EXIST 2023 Task 1](https://clef2023.clef-initiative.eu/index.php?page=Pages/labs.html#EXIST) on sexism detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HWp5bGwySsb"
      },
      "source": [
        "## Problem Definition\n",
        "The systems have to decide whether or not a given tweet contains or describes sexist expressions or behaviors (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behavior).\n",
        "\n",
        "### Examples:\n",
        "\n",
        "**Text**: *Can‚Äôt go a day without women womening*\n",
        "\n",
        "**Label**: Sexist\n",
        "\n",
        "**Text**: *''Society's set norms! Happy men's day though!#weareequal''*\n",
        "\n",
        "**Label**: Not sexist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iu1X4I98M8B"
      },
      "source": [
        "#[Task 1 - 1.0 points] Corpus\n",
        "\n",
        "We have preparared a small version of EXIST dataset in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material/tree/main/2024-2025/Assignment%201/data).\n",
        "\n",
        "Check the `A1/data` folder. It contains 3 `.json` files representing `training`, `validation` and `test` sets.\n",
        "\n",
        "The three sets are slightly unbalanced, with a bias toward the `Non-sexist` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AASoMV9XN5l6"
      },
      "source": [
        "### Dataset Description\n",
        "- The dataset contains tweets in both English and Spanish.\n",
        "- There are labels for multiple tasks, but we are focusing on **Task 1**.\n",
        "- For Task 1, soft labels are assigned by six annotators.\n",
        "- The labels for Task 1 represent whether the tweet is sexist (\"YES\") or not (\"NO\").\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFjwB_lCOQKj"
      },
      "source": [
        "\n",
        "### Example\n",
        "\n",
        "\n",
        "    \"203260\": {\n",
        "        \"id_EXIST\": \"203260\",\n",
        "        \"lang\": \"en\",\n",
        "        \"tweet\": \"ik when mandy says ‚Äúyou look like a whore‚Äù i look cute as FUCK\",\n",
        "        \"number_annotators\": 6,\n",
        "        \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n",
        "        \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n",
        "        \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n",
        "        \"labels_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n",
        "        \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n",
        "        \"labels_task3\": [\n",
        "          [\"STEREOTYPING-DOMINANCE\"],\n",
        "          [\"OBJECTIFICATION\"],\n",
        "          [\"SEXUAL-VIOLENCE\"],\n",
        "          [\"-\"],\n",
        "          [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n",
        "          [\"OBJECTIFICATION\"]\n",
        "        ],\n",
        "        \"split\": \"TRAIN_EN\"\n",
        "      }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QEpzIydTT3YA",
        "outputId": "ac92451f-e5d1-45de-d828-3eb406b5f342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n#Commented because I'm not using jupyter\\nfrom notebook.services.config import ConfigManager\\ncm = ConfigManager()\\ncm.update('livereveal', {\\n        'width': 1024,\\n        'height': 768,\\n        'scroll': True,\\n})\\n\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#0. IMPORTS\n",
        "# file management\n",
        "import urllib\n",
        "from pathlib import Path\n",
        "\n",
        "#zip file\n",
        "import zipfile\n",
        "\n",
        "# dataframe management\n",
        "import pandas as pd\n",
        "\n",
        "# data manipulation\n",
        "import numpy as np\n",
        "\n",
        "# for readability\n",
        "from typing import Iterable\n",
        "\n",
        "# viz\n",
        "from tqdm import tqdm\n",
        "'''\n",
        "#Commented because I'm not using jupyter\n",
        "from notebook.services.config import ConfigManager\n",
        "cm = ConfigManager()\n",
        "cm.update('livereveal', {\n",
        "        'width': 1024,\n",
        "        'height': 768,\n",
        "        'scroll': True,\n",
        "})\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ45bvuOOJ7I"
      },
      "source": [
        "### Instructions\n",
        "1. **Download** the `A1/data` folder.\n",
        "2. **Load** the three JSON files and encode them as pandas dataframes.\n",
        "3. **Generate hard labels** for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset.\n",
        "4. **Filter the DataFrame** to keep only rows where the `lang` column is `'en'`.\n",
        "5. **Remove unwanted columns**: Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`.\n",
        "6. **Encode the `hard_label_task1` column**: Use 1 to represent \"YES\" and 0 to represent \"NO\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXsIQAGrTs2i"
      },
      "source": [
        "### 1. Download the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sWsENHaATpP1"
      },
      "outputs": [],
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    \"\"\"\n",
        "    A class to represent a download progress bar, extending the tqdm class.\n",
        "    \"\"\"\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(download_path: Path, url: str):\n",
        "    \"\"\"\n",
        "    Downloads a file from the given URL to the specified download path, displaying a progress bar.\n",
        "    \"\"\"\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b4rZelPCT52F"
      },
      "outputs": [],
      "source": [
        "def download_dataset(download_path: Path, url: str):\n",
        "    \"\"\"\n",
        "    Downloads a dataset from a given URL to a specified download path.\n",
        "    \"\"\"\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_url(url=url, download_path=download_path)\n",
        "    print(\"Download complete!\")\n",
        "\n",
        "\n",
        "def extract_dataset(download_path: Path, extract_path: Path):\n",
        "    \"\"\"\n",
        "    Extracts a dataset from a ZIP file.\n",
        "\n",
        "    This function checks if the specified download path points to a ZIP file.\n",
        "    If it does, the function extracts the contents of the ZIP file to the specified\n",
        "    extraction path. If the file is not a ZIP file, an error message is printed.\n",
        "    \"\"\"\n",
        "    print(\"Extracting dataset... (it may take a while...)\")\n",
        "    # Check if the file is a ZIP file\n",
        "    if zipfile.is_zipfile(download_path):\n",
        "        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(\"Extraction completed!\")\n",
        "    else:\n",
        "        print(\"Error: The downloaded file is not a ZIP file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPTJJ8cnT8DZ",
        "outputId": "37e2f97d-bbc8-43cd-afd0-1cfad73e277e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "main.zip: 4.58MB [00:06, 749kB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete!\n",
            "Extracting dataset... (it may take a while...)\n",
            "Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "# Define paths and URL\n",
        "url = \"https://github.com/nlp-unibo/nlp-course-material/archive/refs/heads/main.zip\"\n",
        "dataset_name = \"Exist\"\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
        "dataset_folder.mkdir(exist_ok=True)  # Create folder if it doesn't exist\n",
        "download_path = dataset_folder.joinpath(f\"{dataset_name}.zip\")\n",
        "\n",
        "# Download and extract\n",
        "download_dataset(download_path, url)\n",
        "extract_dataset(download_path, dataset_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go31XBMvULGq"
      },
      "source": [
        "### 2. Load the three JSON files and encode them as pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-4iL6MnUQNz",
        "outputId": "8a176e32-5de0-4573-8458-993ba51649e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training.json loaded successfully as a DataFrame!\n",
            "training.json DataFrame shape: (11, 6920)\n",
            "test.json loaded successfully as a DataFrame!\n",
            "test.json DataFrame shape: (11, 312)\n",
            "validation.json loaded successfully as a DataFrame!\n",
            "validation.json DataFrame shape: (11, 726)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path to the data folder and load JSON files as DataFrames\n",
        "data_folder = dataset_folder.joinpath(\"nlp-course-material-main\", \"2024-2025\", \"Assignment 1\", \"data\")\n",
        "\n",
        "# Load each JSON file as a DataFrame\n",
        "training_file = data_folder.joinpath(\"training.json\")\n",
        "test_file = data_folder.joinpath(\"test.json\")\n",
        "validation_file = data_folder.joinpath(\"validation.json\")\n",
        "\"\"\"\n",
        "def load_json_file(file_path: Path) -> pd.DataFrame:\n",
        "    Loads a JSON file as a DataFrame.\n",
        "    \n",
        "    with open(file_path, \"r\") as file:\n",
        "        df = pd.read_json(file)\n",
        "        print(f\"{file_path.name} loaded successfully as a DataFrame!\")\n",
        "        print(f\"{file_path.name} DataFrame shape: {df.shape}\")\n",
        "    return df\n",
        "\"\"\"\n",
        "def load_json_file(file_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads a JSON file as a DataFrame.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        df = pd.read_json(file)\n",
        "        print(f\"{file_path.name} loaded successfully as a DataFrame!\")\n",
        "        print(f\"{file_path.name} DataFrame shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "training_df = load_json_file(training_file)\n",
        "test_df = load_json_file(test_file)\n",
        "validation_df = load_json_file(validation_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "ksjLz58AUVnp",
        "outputId": "8c835e10-c9b5-45e5-9649-6e2a222ecef8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>100001</th>\n",
              "      <th>100002</th>\n",
              "      <th>100003</th>\n",
              "      <th>100004</th>\n",
              "      <th>100005</th>\n",
              "      <th>100006</th>\n",
              "      <th>100007</th>\n",
              "      <th>100008</th>\n",
              "      <th>100009</th>\n",
              "      <th>100010</th>\n",
              "      <th>...</th>\n",
              "      <th>203251</th>\n",
              "      <th>203252</th>\n",
              "      <th>203253</th>\n",
              "      <th>203254</th>\n",
              "      <th>203255</th>\n",
              "      <th>203256</th>\n",
              "      <th>203257</th>\n",
              "      <th>203258</th>\n",
              "      <th>203259</th>\n",
              "      <th>203260</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <td>100001</td>\n",
              "      <td>100002</td>\n",
              "      <td>100003</td>\n",
              "      <td>100004</td>\n",
              "      <td>100005</td>\n",
              "      <td>100006</td>\n",
              "      <td>100007</td>\n",
              "      <td>100008</td>\n",
              "      <td>100009</td>\n",
              "      <td>100010</td>\n",
              "      <td>...</td>\n",
              "      <td>203251</td>\n",
              "      <td>203252</td>\n",
              "      <td>203253</td>\n",
              "      <td>203254</td>\n",
              "      <td>203255</td>\n",
              "      <td>203256</td>\n",
              "      <td>203257</td>\n",
              "      <td>203258</td>\n",
              "      <td>203259</td>\n",
              "      <td>203260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang</th>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>...</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet</th>\n",
              "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
              "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
              "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
              "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
              "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
              "      <td>@yonkykong Aaah s√≠. Andrew Dobson. El que se d...</td>\n",
              "      <td>@glutamatom @JoaquinAdolfoC Estaba del lado de...</td>\n",
              "      <td>@BestKabest Esta gringa sigue llorando por el ...</td>\n",
              "      <td>.¬øConoces la #DECORACION #estilo #GAMER para #...</td>\n",
              "      <td>CES 2022 ASUS ROG Rise of Gamers Evento de lan...</td>\n",
              "      <td>...</td>\n",
              "      <td>\"you look like a whore\" I'm literally wearing ...</td>\n",
              "      <td>‚ÄúYou look like a whore‚Äù if you think I‚Äôm cute ...</td>\n",
              "      <td>Who fucking lied to you? You look like a whore...</td>\n",
              "      <td>@ShefVaidya Ma'am if I say that you look like ...</td>\n",
              "      <td>I forgot I have a m*d that changes the drachen...</td>\n",
              "      <td>idk why y‚Äôall bitches think having half your a...</td>\n",
              "      <td>This has been a part of an experiment with @Wo...</td>\n",
              "      <td>\"Take me already\" \"Not yet. You gotta be ready...</td>\n",
              "      <td>@clintneedcoffee why do you look like a whore?...</td>\n",
              "      <td>ik when mandy says ‚Äúyou look like a whore‚Äù i l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_annotators</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annotators</th>\n",
              "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
              "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
              "      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n",
              "      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n",
              "      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n",
              "      <td>[Annotator_31, Annotator_32, Annotator_33, Ann...</td>\n",
              "      <td>[Annotator_37, Annotator_38, Annotator_39, Ann...</td>\n",
              "      <td>...</td>\n",
              "      <td>[Annotator_473, Annotator_474, Annotator_475, ...</td>\n",
              "      <td>[Annotator_617, Annotator_618, Annotator_619, ...</td>\n",
              "      <td>[Annotator_617, Annotator_618, Annotator_619, ...</td>\n",
              "      <td>[Annotator_668, Annotator_669, Annotator_670, ...</td>\n",
              "      <td>[Annotator_674, Annotator_675, Annotator_676, ...</td>\n",
              "      <td>[Annotator_478, Annotator_479, Annotator_480, ...</td>\n",
              "      <td>[Annotator_668, Annotator_669, Annotator_670, ...</td>\n",
              "      <td>[Annotator_467, Annotator_468, Annotator_469, ...</td>\n",
              "      <td>[Annotator_674, Annotator_675, Annotator_676, ...</td>\n",
              "      <td>[Annotator_473, Annotator_474, Annotator_475, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 6920 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                              100001  \\\n",
              "id_EXIST                                                      100001   \n",
              "lang                                                              es   \n",
              "tweet              @TheChiflis Ignora al otro, es un capullo.El p...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
              "\n",
              "                                                              100002  \\\n",
              "id_EXIST                                                      100002   \n",
              "lang                                                              es   \n",
              "tweet              @ultimonomada_ Si comicsgate se parece en algo...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "\n",
              "                                                              100003  \\\n",
              "id_EXIST                                                      100003   \n",
              "lang                                                              es   \n",
              "tweet              @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "\n",
              "                                                              100004  \\\n",
              "id_EXIST                                                      100004   \n",
              "lang                                                              es   \n",
              "tweet              @Lunariita7 Un retraso social bastante lamenta...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
              "\n",
              "                                                              100005  \\\n",
              "id_EXIST                                                      100005   \n",
              "lang                                                              es   \n",
              "tweet              @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
              "\n",
              "                                                              100006  \\\n",
              "id_EXIST                                                      100006   \n",
              "lang                                                              es   \n",
              "tweet              @yonkykong Aaah s√≠. Andrew Dobson. El que se d...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
              "\n",
              "                                                              100007  \\\n",
              "id_EXIST                                                      100007   \n",
              "lang                                                              es   \n",
              "tweet              @glutamatom @JoaquinAdolfoC Estaba del lado de...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
              "\n",
              "                                                              100008  \\\n",
              "id_EXIST                                                      100008   \n",
              "lang                                                              es   \n",
              "tweet              @BestKabest Esta gringa sigue llorando por el ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
              "\n",
              "                                                              100009  \\\n",
              "id_EXIST                                                      100009   \n",
              "lang                                                              es   \n",
              "tweet              .¬øConoces la #DECORACION #estilo #GAMER para #...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_31, Annotator_32, Annotator_33, Ann...   \n",
              "\n",
              "                                                              100010  ...  \\\n",
              "id_EXIST                                                      100010  ...   \n",
              "lang                                                              es  ...   \n",
              "tweet              CES 2022 ASUS ROG Rise of Gamers Evento de lan...  ...   \n",
              "number_annotators                                                  6  ...   \n",
              "annotators         [Annotator_37, Annotator_38, Annotator_39, Ann...  ...   \n",
              "\n",
              "                                                              203251  \\\n",
              "id_EXIST                                                      203251   \n",
              "lang                                                              en   \n",
              "tweet              \"you look like a whore\" I'm literally wearing ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_473, Annotator_474, Annotator_475, ...   \n",
              "\n",
              "                                                              203252  \\\n",
              "id_EXIST                                                      203252   \n",
              "lang                                                              en   \n",
              "tweet              ‚ÄúYou look like a whore‚Äù if you think I‚Äôm cute ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_617, Annotator_618, Annotator_619, ...   \n",
              "\n",
              "                                                              203253  \\\n",
              "id_EXIST                                                      203253   \n",
              "lang                                                              en   \n",
              "tweet              Who fucking lied to you? You look like a whore...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_617, Annotator_618, Annotator_619, ...   \n",
              "\n",
              "                                                              203254  \\\n",
              "id_EXIST                                                      203254   \n",
              "lang                                                              en   \n",
              "tweet              @ShefVaidya Ma'am if I say that you look like ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_668, Annotator_669, Annotator_670, ...   \n",
              "\n",
              "                                                              203255  \\\n",
              "id_EXIST                                                      203255   \n",
              "lang                                                              en   \n",
              "tweet              I forgot I have a m*d that changes the drachen...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_674, Annotator_675, Annotator_676, ...   \n",
              "\n",
              "                                                              203256  \\\n",
              "id_EXIST                                                      203256   \n",
              "lang                                                              en   \n",
              "tweet              idk why y‚Äôall bitches think having half your a...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_478, Annotator_479, Annotator_480, ...   \n",
              "\n",
              "                                                              203257  \\\n",
              "id_EXIST                                                      203257   \n",
              "lang                                                              en   \n",
              "tweet              This has been a part of an experiment with @Wo...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_668, Annotator_669, Annotator_670, ...   \n",
              "\n",
              "                                                              203258  \\\n",
              "id_EXIST                                                      203258   \n",
              "lang                                                              en   \n",
              "tweet              \"Take me already\" \"Not yet. You gotta be ready...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_467, Annotator_468, Annotator_469, ...   \n",
              "\n",
              "                                                              203259  \\\n",
              "id_EXIST                                                      203259   \n",
              "lang                                                              en   \n",
              "tweet              @clintneedcoffee why do you look like a whore?...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_674, Annotator_675, Annotator_676, ...   \n",
              "\n",
              "                                                              203260  \n",
              "id_EXIST                                                      203260  \n",
              "lang                                                              en  \n",
              "tweet              ik when mandy says ‚Äúyou look like a whore‚Äù i l...  \n",
              "number_annotators                                                  6  \n",
              "annotators         [Annotator_473, Annotator_474, Annotator_475, ...  \n",
              "\n",
              "[5 rows x 6920 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "SIzuv3YzKtKM",
        "outputId": "7daa3ac6-1bdb-4504-aa55-100ef688d53b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>400178</th>\n",
              "      <th>400179</th>\n",
              "      <th>400180</th>\n",
              "      <th>400181</th>\n",
              "      <th>400182</th>\n",
              "      <th>400183</th>\n",
              "      <th>400184</th>\n",
              "      <th>400185</th>\n",
              "      <th>400186</th>\n",
              "      <th>400187</th>\n",
              "      <th>...</th>\n",
              "      <th>400480</th>\n",
              "      <th>400481</th>\n",
              "      <th>400482</th>\n",
              "      <th>400483</th>\n",
              "      <th>400484</th>\n",
              "      <th>400485</th>\n",
              "      <th>400486</th>\n",
              "      <th>400487</th>\n",
              "      <th>400488</th>\n",
              "      <th>400489</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <td>400178</td>\n",
              "      <td>400179</td>\n",
              "      <td>400180</td>\n",
              "      <td>400181</td>\n",
              "      <td>400182</td>\n",
              "      <td>400183</td>\n",
              "      <td>400184</td>\n",
              "      <td>400185</td>\n",
              "      <td>400186</td>\n",
              "      <td>400187</td>\n",
              "      <td>...</td>\n",
              "      <td>400480</td>\n",
              "      <td>400481</td>\n",
              "      <td>400482</td>\n",
              "      <td>400483</td>\n",
              "      <td>400484</td>\n",
              "      <td>400485</td>\n",
              "      <td>400486</td>\n",
              "      <td>400487</td>\n",
              "      <td>400488</td>\n",
              "      <td>400489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet</th>\n",
              "      <td>1st day at the pool on a beautiful Sunday in N...</td>\n",
              "      <td>‚ÄúI like your outfit too except when i dress up...</td>\n",
              "      <td>@KNasFanFic ü•∫üíñ same, though!!! the angst just ...</td>\n",
              "      <td>@themaxburns @GOP Fuck that cunt. Tried to vot...</td>\n",
              "      <td>@ultshunnie u gotta say some shit like ‚Äúi‚Äôll f...</td>\n",
              "      <td>@FernsLewds ‚ÄúMmm, fuck that cunt is hungry for...</td>\n",
              "      <td>Suika gets gangbanged to death https://t.co/vn...</td>\n",
              "      <td>‚ÄúI mean I do, but‚Ä¶ wouldn‚Äôt it be fun to get g...</td>\n",
              "      <td>@Fox_x_Gods Maybe something involving her gett...</td>\n",
              "      <td>@LogicalFallac18 @LBC Sex* as in 'gender' hara...</td>\n",
              "      <td>...</td>\n",
              "      <td>i hate liarsüòÇ. specifically those who lie bc t...</td>\n",
              "      <td>Uh, they're allowed to? calling this out makes...</td>\n",
              "      <td>This genration are no more then selfish cunts....</td>\n",
              "      <td>@JetCelestial @polgara28951124 @CharlotteEmmaU...</td>\n",
              "      <td>@TheRealLyna Bitch your boyfriend Anthony flim...</td>\n",
              "      <td>@YesReallyAngel ‚ÄúDon‚Äôt wear a black bra with a...</td>\n",
              "      <td>\" get changed , you look like a prostitute . \"...</td>\n",
              "      <td>made this top and my mom gave me the ‚Äúyou look...</td>\n",
              "      <td>@DawnAnd91320913 I haven't seen anything that ...</td>\n",
              "      <td>@ElDukemane You look like a whore in ur new pi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_annotators</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annotators</th>\n",
              "      <td>[Annotator_764, Annotator_765, Annotator_766, ...</td>\n",
              "      <td>[Annotator_805, Annotator_426, Annotator_806, ...</td>\n",
              "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
              "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
              "      <td>[Annotator_770, Annotator_771, Annotator_772, ...</td>\n",
              "      <td>[Annotator_776, Annotator_777, Annotator_195, ...</td>\n",
              "      <td>[Annotator_780, Annotator_781, Annotator_782, ...</td>\n",
              "      <td>[Annotator_785, Annotator_786, Annotator_787, ...</td>\n",
              "      <td>[Annotator_770, Annotator_771, Annotator_772, ...</td>\n",
              "      <td>[Annotator_791, Annotator_122, Annotator_396, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[Annotator_776, Annotator_777, Annotator_195, ...</td>\n",
              "      <td>[Annotator_805, Annotator_426, Annotator_806, ...</td>\n",
              "      <td>[Annotator_776, Annotator_777, Annotator_195, ...</td>\n",
              "      <td>[Annotator_801, Annotator_182, Annotator_802, ...</td>\n",
              "      <td>[Annotator_801, Annotator_182, Annotator_802, ...</td>\n",
              "      <td>[Annotator_801, Annotator_182, Annotator_802, ...</td>\n",
              "      <td>[Annotator_801, Annotator_182, Annotator_802, ...</td>\n",
              "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
              "      <td>[Annotator_776, Annotator_777, Annotator_195, ...</td>\n",
              "      <td>[Annotator_776, Annotator_777, Annotator_195, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 312 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                              400178  \\\n",
              "id_EXIST                                                      400178   \n",
              "lang                                                              en   \n",
              "tweet              1st day at the pool on a beautiful Sunday in N...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_764, Annotator_765, Annotator_766, ...   \n",
              "\n",
              "                                                              400179  \\\n",
              "id_EXIST                                                      400179   \n",
              "lang                                                              en   \n",
              "tweet              ‚ÄúI like your outfit too except when i dress up...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_805, Annotator_426, Annotator_806, ...   \n",
              "\n",
              "                                                              400180  \\\n",
              "id_EXIST                                                      400180   \n",
              "lang                                                              en   \n",
              "tweet              @KNasFanFic ü•∫üíñ same, though!!! the angst just ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_795, Annotator_796, Annotator_797, ...   \n",
              "\n",
              "                                                              400181  \\\n",
              "id_EXIST                                                      400181   \n",
              "lang                                                              en   \n",
              "tweet              @themaxburns @GOP Fuck that cunt. Tried to vot...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_795, Annotator_796, Annotator_797, ...   \n",
              "\n",
              "                                                              400182  \\\n",
              "id_EXIST                                                      400182   \n",
              "lang                                                              en   \n",
              "tweet              @ultshunnie u gotta say some shit like ‚Äúi‚Äôll f...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_770, Annotator_771, Annotator_772, ...   \n",
              "\n",
              "                                                              400183  \\\n",
              "id_EXIST                                                      400183   \n",
              "lang                                                              en   \n",
              "tweet              @FernsLewds ‚ÄúMmm, fuck that cunt is hungry for...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_776, Annotator_777, Annotator_195, ...   \n",
              "\n",
              "                                                              400184  \\\n",
              "id_EXIST                                                      400184   \n",
              "lang                                                              en   \n",
              "tweet              Suika gets gangbanged to death https://t.co/vn...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_780, Annotator_781, Annotator_782, ...   \n",
              "\n",
              "                                                              400185  \\\n",
              "id_EXIST                                                      400185   \n",
              "lang                                                              en   \n",
              "tweet              ‚ÄúI mean I do, but‚Ä¶ wouldn‚Äôt it be fun to get g...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_785, Annotator_786, Annotator_787, ...   \n",
              "\n",
              "                                                              400186  \\\n",
              "id_EXIST                                                      400186   \n",
              "lang                                                              en   \n",
              "tweet              @Fox_x_Gods Maybe something involving her gett...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_770, Annotator_771, Annotator_772, ...   \n",
              "\n",
              "                                                              400187  ...  \\\n",
              "id_EXIST                                                      400187  ...   \n",
              "lang                                                              en  ...   \n",
              "tweet              @LogicalFallac18 @LBC Sex* as in 'gender' hara...  ...   \n",
              "number_annotators                                                  6  ...   \n",
              "annotators         [Annotator_791, Annotator_122, Annotator_396, ...  ...   \n",
              "\n",
              "                                                              400480  \\\n",
              "id_EXIST                                                      400480   \n",
              "lang                                                              en   \n",
              "tweet              i hate liarsüòÇ. specifically those who lie bc t...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_776, Annotator_777, Annotator_195, ...   \n",
              "\n",
              "                                                              400481  \\\n",
              "id_EXIST                                                      400481   \n",
              "lang                                                              en   \n",
              "tweet              Uh, they're allowed to? calling this out makes...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_805, Annotator_426, Annotator_806, ...   \n",
              "\n",
              "                                                              400482  \\\n",
              "id_EXIST                                                      400482   \n",
              "lang                                                              en   \n",
              "tweet              This genration are no more then selfish cunts....   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_776, Annotator_777, Annotator_195, ...   \n",
              "\n",
              "                                                              400483  \\\n",
              "id_EXIST                                                      400483   \n",
              "lang                                                              en   \n",
              "tweet              @JetCelestial @polgara28951124 @CharlotteEmmaU...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_801, Annotator_182, Annotator_802, ...   \n",
              "\n",
              "                                                              400484  \\\n",
              "id_EXIST                                                      400484   \n",
              "lang                                                              en   \n",
              "tweet              @TheRealLyna Bitch your boyfriend Anthony flim...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_801, Annotator_182, Annotator_802, ...   \n",
              "\n",
              "                                                              400485  \\\n",
              "id_EXIST                                                      400485   \n",
              "lang                                                              en   \n",
              "tweet              @YesReallyAngel ‚ÄúDon‚Äôt wear a black bra with a...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_801, Annotator_182, Annotator_802, ...   \n",
              "\n",
              "                                                              400486  \\\n",
              "id_EXIST                                                      400486   \n",
              "lang                                                              en   \n",
              "tweet              \" get changed , you look like a prostitute . \"...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_801, Annotator_182, Annotator_802, ...   \n",
              "\n",
              "                                                              400487  \\\n",
              "id_EXIST                                                      400487   \n",
              "lang                                                              en   \n",
              "tweet              made this top and my mom gave me the ‚Äúyou look...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_795, Annotator_796, Annotator_797, ...   \n",
              "\n",
              "                                                              400488  \\\n",
              "id_EXIST                                                      400488   \n",
              "lang                                                              en   \n",
              "tweet              @DawnAnd91320913 I haven't seen anything that ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_776, Annotator_777, Annotator_195, ...   \n",
              "\n",
              "                                                              400489  \n",
              "id_EXIST                                                      400489  \n",
              "lang                                                              en  \n",
              "tweet              @ElDukemane You look like a whore in ur new pi...  \n",
              "number_annotators                                                  6  \n",
              "annotators         [Annotator_776, Annotator_777, Annotator_195, ...  \n",
              "\n",
              "[5 rows x 312 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "CbGMaitUK8S-",
        "outputId": "a97020a7-ca10-48e6-8b4b-b2b74f2bf05a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>300001</th>\n",
              "      <th>300002</th>\n",
              "      <th>300003</th>\n",
              "      <th>300004</th>\n",
              "      <th>300005</th>\n",
              "      <th>300006</th>\n",
              "      <th>300007</th>\n",
              "      <th>300008</th>\n",
              "      <th>300009</th>\n",
              "      <th>300010</th>\n",
              "      <th>...</th>\n",
              "      <th>400168</th>\n",
              "      <th>400169</th>\n",
              "      <th>400170</th>\n",
              "      <th>400171</th>\n",
              "      <th>400172</th>\n",
              "      <th>400173</th>\n",
              "      <th>400174</th>\n",
              "      <th>400175</th>\n",
              "      <th>400176</th>\n",
              "      <th>400177</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <td>300001</td>\n",
              "      <td>300002</td>\n",
              "      <td>300003</td>\n",
              "      <td>300004</td>\n",
              "      <td>300005</td>\n",
              "      <td>300006</td>\n",
              "      <td>300007</td>\n",
              "      <td>300008</td>\n",
              "      <td>300009</td>\n",
              "      <td>300010</td>\n",
              "      <td>...</td>\n",
              "      <td>400168</td>\n",
              "      <td>400169</td>\n",
              "      <td>400170</td>\n",
              "      <td>400171</td>\n",
              "      <td>400172</td>\n",
              "      <td>400173</td>\n",
              "      <td>400174</td>\n",
              "      <td>400175</td>\n",
              "      <td>400176</td>\n",
              "      <td>400177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang</th>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>...</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet</th>\n",
              "      <td>@Fichinescu La comunidad gamer es un antro de ...</td>\n",
              "      <td>@anacaotica88 @MordorLivin No me acuerdo de lo...</td>\n",
              "      <td>@cosmicJunkBot lo digo cada pocos dias y lo re...</td>\n",
              "      <td>Also mientras les decia eso la se√±alaba y deci...</td>\n",
              "      <td>And all people killed,  attacked, harassed by ...</td>\n",
              "      <td>On this #WorldPressFreedomDay I‚Äôm thinking of ...</td>\n",
              "      <td>@DavidGR18 @pppbernat @abc_es @agarzon @IreneM...</td>\n",
              "      <td>@DavidArranzVox @AnabelAlonso_of Uyyy a q huel...</td>\n",
              "      <td>Con 25 Leonesü¶Å y 500 m√°s en las gradasüó£!!#EkoF...</td>\n",
              "      <td>@kokreto84 @Play87834898 @venusoncrack Me gust...</td>\n",
              "      <td>...</td>\n",
              "      <td>I'm debating doing a \"feminization\" clip serie...</td>\n",
              "      <td>I'm looking for a girl I spoke to the day befo...</td>\n",
              "      <td>@parker__farquer Three for a girl.You're going...</td>\n",
              "      <td>Foreigner - Waiting for a Girl Like You [Lyric...</td>\n",
              "      <td>@leesu44 @elishabroadway @markbann57 @SeaeyesT...</td>\n",
              "      <td>Amazing that the GOP is trying to take away ou...</td>\n",
              "      <td>It is is impossible for a man to become a woma...</td>\n",
              "      <td>If Gaga decided to sing 18 versions of Free Wo...</td>\n",
              "      <td>This is your reminder that you can be child-fr...</td>\n",
              "      <td>just completed my last final, i‚Äôm officially a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_annotators</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annotators</th>\n",
              "      <td>[Annotator_726, Annotator_727, Annotator_357, ...</td>\n",
              "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
              "      <td>[Annotator_735, Annotator_736, Annotator_345, ...</td>\n",
              "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
              "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
              "      <td>[Annotator_735, Annotator_736, Annotator_345, ...</td>\n",
              "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
              "      <td>[Annotator_742, Annotator_743, Annotator_195, ...</td>\n",
              "      <td>[Annotator_742, Annotator_743, Annotator_195, ...</td>\n",
              "      <td>[Annotator_744, Annotator_745, Annotator_746, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[Annotator_805, Annotator_426, Annotator_806, ...</td>\n",
              "      <td>[Annotator_780, Annotator_781, Annotator_782, ...</td>\n",
              "      <td>[Annotator_785, Annotator_786, Annotator_787, ...</td>\n",
              "      <td>[Annotator_764, Annotator_765, Annotator_766, ...</td>\n",
              "      <td>[Annotator_780, Annotator_781, Annotator_782, ...</td>\n",
              "      <td>[Annotator_805, Annotator_426, Annotator_806, ...</td>\n",
              "      <td>[Annotator_770, Annotator_771, Annotator_772, ...</td>\n",
              "      <td>[Annotator_764, Annotator_765, Annotator_766, ...</td>\n",
              "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
              "      <td>[Annotator_770, Annotator_771, Annotator_772, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 726 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                              300001  \\\n",
              "id_EXIST                                                      300001   \n",
              "lang                                                              es   \n",
              "tweet              @Fichinescu La comunidad gamer es un antro de ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_726, Annotator_727, Annotator_357, ...   \n",
              "\n",
              "                                                              300002  \\\n",
              "id_EXIST                                                      300002   \n",
              "lang                                                              es   \n",
              "tweet              @anacaotica88 @MordorLivin No me acuerdo de lo...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_731, Annotator_732, Annotator_315, ...   \n",
              "\n",
              "                                                              300003  \\\n",
              "id_EXIST                                                      300003   \n",
              "lang                                                              es   \n",
              "tweet              @cosmicJunkBot lo digo cada pocos dias y lo re...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_735, Annotator_736, Annotator_345, ...   \n",
              "\n",
              "                                                              300004  \\\n",
              "id_EXIST                                                      300004   \n",
              "lang                                                              es   \n",
              "tweet              Also mientras les decia eso la se√±alaba y deci...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_259, Annotator_739, Annotator_291, ...   \n",
              "\n",
              "                                                              300005  \\\n",
              "id_EXIST                                                      300005   \n",
              "lang                                                              es   \n",
              "tweet              And all people killed,  attacked, harassed by ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_731, Annotator_732, Annotator_315, ...   \n",
              "\n",
              "                                                              300006  \\\n",
              "id_EXIST                                                      300006   \n",
              "lang                                                              es   \n",
              "tweet              On this #WorldPressFreedomDay I‚Äôm thinking of ...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_735, Annotator_736, Annotator_345, ...   \n",
              "\n",
              "                                                              300007  \\\n",
              "id_EXIST                                                      300007   \n",
              "lang                                                              es   \n",
              "tweet              @DavidGR18 @pppbernat @abc_es @agarzon @IreneM...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_731, Annotator_732, Annotator_315, ...   \n",
              "\n",
              "                                                              300008  \\\n",
              "id_EXIST                                                      300008   \n",
              "lang                                                              es   \n",
              "tweet              @DavidArranzVox @AnabelAlonso_of Uyyy a q huel...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_742, Annotator_743, Annotator_195, ...   \n",
              "\n",
              "                                                              300009  \\\n",
              "id_EXIST                                                      300009   \n",
              "lang                                                              es   \n",
              "tweet              Con 25 Leonesü¶Å y 500 m√°s en las gradasüó£!!#EkoF...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_742, Annotator_743, Annotator_195, ...   \n",
              "\n",
              "                                                              300010  ...  \\\n",
              "id_EXIST                                                      300010  ...   \n",
              "lang                                                              es  ...   \n",
              "tweet              @kokreto84 @Play87834898 @venusoncrack Me gust...  ...   \n",
              "number_annotators                                                  6  ...   \n",
              "annotators         [Annotator_744, Annotator_745, Annotator_746, ...  ...   \n",
              "\n",
              "                                                              400168  \\\n",
              "id_EXIST                                                      400168   \n",
              "lang                                                              en   \n",
              "tweet              I'm debating doing a \"feminization\" clip serie...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_805, Annotator_426, Annotator_806, ...   \n",
              "\n",
              "                                                              400169  \\\n",
              "id_EXIST                                                      400169   \n",
              "lang                                                              en   \n",
              "tweet              I'm looking for a girl I spoke to the day befo...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_780, Annotator_781, Annotator_782, ...   \n",
              "\n",
              "                                                              400170  \\\n",
              "id_EXIST                                                      400170   \n",
              "lang                                                              en   \n",
              "tweet              @parker__farquer Three for a girl.You're going...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_785, Annotator_786, Annotator_787, ...   \n",
              "\n",
              "                                                              400171  \\\n",
              "id_EXIST                                                      400171   \n",
              "lang                                                              en   \n",
              "tweet              Foreigner - Waiting for a Girl Like You [Lyric...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_764, Annotator_765, Annotator_766, ...   \n",
              "\n",
              "                                                              400172  \\\n",
              "id_EXIST                                                      400172   \n",
              "lang                                                              en   \n",
              "tweet              @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_780, Annotator_781, Annotator_782, ...   \n",
              "\n",
              "                                                              400173  \\\n",
              "id_EXIST                                                      400173   \n",
              "lang                                                              en   \n",
              "tweet              Amazing that the GOP is trying to take away ou...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_805, Annotator_426, Annotator_806, ...   \n",
              "\n",
              "                                                              400174  \\\n",
              "id_EXIST                                                      400174   \n",
              "lang                                                              en   \n",
              "tweet              It is is impossible for a man to become a woma...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_770, Annotator_771, Annotator_772, ...   \n",
              "\n",
              "                                                              400175  \\\n",
              "id_EXIST                                                      400175   \n",
              "lang                                                              en   \n",
              "tweet              If Gaga decided to sing 18 versions of Free Wo...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_764, Annotator_765, Annotator_766, ...   \n",
              "\n",
              "                                                              400176  \\\n",
              "id_EXIST                                                      400176   \n",
              "lang                                                              en   \n",
              "tweet              This is your reminder that you can be child-fr...   \n",
              "number_annotators                                                  6   \n",
              "annotators         [Annotator_795, Annotator_796, Annotator_797, ...   \n",
              "\n",
              "                                                              400177  \n",
              "id_EXIST                                                      400177  \n",
              "lang                                                              en  \n",
              "tweet              just completed my last final, i‚Äôm officially a...  \n",
              "number_annotators                                                  6  \n",
              "annotators         [Annotator_770, Annotator_771, Annotator_772, ...  \n",
              "\n",
              "[5 rows x 726 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BRg9ErqIUZnf"
      },
      "outputs": [],
      "source": [
        "training_df = training_df.transpose().set_index(\"id_EXIST\")\n",
        "test_df = test_df.transpose().set_index(\"id_EXIST\")\n",
        "validation_df = validation_df.transpose().set_index(\"id_EXIST\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "hK7cPTCGUau3",
        "outputId": "34851d27-d322-49ce-f88b-ec6c5183247a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>number_annotators</th>\n",
              "      <th>annotators</th>\n",
              "      <th>gender_annotators</th>\n",
              "      <th>age_annotators</th>\n",
              "      <th>labels_task1</th>\n",
              "      <th>labels_task2</th>\n",
              "      <th>labels_task3</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100001</th>\n",
              "      <td>es</td>\n",
              "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
              "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
              "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100002</th>\n",
              "      <td>es</td>\n",
              "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
              "      <td>[-, -, -, -, DIRECT, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100003</th>\n",
              "      <td>es</td>\n",
              "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
              "      <td>[-, -, -, -, -, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100004</th>\n",
              "      <td>es</td>\n",
              "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
              "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
              "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100005</th>\n",
              "      <td>es</td>\n",
              "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
              "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
              "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "100001     es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
              "100002     es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
              "100003     es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
              "100004     es  @Lunariita7 Un retraso social bastante lamenta...   \n",
              "100005     es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
              "\n",
              "         number_annotators                                         annotators  \\\n",
              "id_EXIST                                                                        \n",
              "100001                   6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
              "100002                   6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "100003                   6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "100004                   6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
              "100005                   6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
              "\n",
              "           gender_annotators                          age_annotators  \\\n",
              "id_EXIST                                                               \n",
              "100001    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100002    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100003    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100004    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100005    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "\n",
              "                           labels_task1  \\\n",
              "id_EXIST                                  \n",
              "100001    [YES, YES, NO, YES, YES, YES]   \n",
              "100002        [NO, NO, NO, NO, YES, NO]   \n",
              "100003         [NO, NO, NO, NO, NO, NO]   \n",
              "100004      [NO, NO, YES, NO, YES, YES]   \n",
              "100005     [YES, NO, YES, NO, YES, YES]   \n",
              "\n",
              "                                               labels_task2  \\\n",
              "id_EXIST                                                      \n",
              "100001    [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
              "100002                              [-, -, -, -, DIRECT, -]   \n",
              "100003                                   [-, -, -, -, -, -]   \n",
              "100004                [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
              "100005    [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
              "\n",
              "                                               labels_task3     split  \n",
              "id_EXIST                                                               \n",
              "100001    [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
              "100002         [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
              "100003                       [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
              "100004    [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
              "100005    [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "CUyCWFtOK1Fb",
        "outputId": "538c73ff-dfae-4801-e2d2-e1e2b795a410"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>number_annotators</th>\n",
              "      <th>annotators</th>\n",
              "      <th>gender_annotators</th>\n",
              "      <th>age_annotators</th>\n",
              "      <th>labels_task1</th>\n",
              "      <th>labels_task2</th>\n",
              "      <th>labels_task3</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>400178</th>\n",
              "      <td>en</td>\n",
              "      <td>1st day at the pool on a beautiful Sunday in N...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_764, Annotator_765, Annotator_766, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
              "      <td>[-, -, -, -, -, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
              "      <td>DEV_EN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400179</th>\n",
              "      <td>en</td>\n",
              "      <td>‚ÄúI like your outfit too except when i dress up...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_805, Annotator_426, Annotator_806, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[YES, YES, YES, YES, YES, NO]</td>\n",
              "      <td>[JUDGEMENTAL, DIRECT, REPORTED, DIRECT, REPORT...</td>\n",
              "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, MISOGYNY...</td>\n",
              "      <td>DEV_EN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400180</th>\n",
              "      <td>en</td>\n",
              "      <td>@KNasFanFic ü•∫üíñ same, though!!! the angst just ...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
              "      <td>[-, -, -, -, -, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
              "      <td>DEV_EN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400181</th>\n",
              "      <td>en</td>\n",
              "      <td>@themaxburns @GOP Fuck that cunt. Tried to vot...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
              "      <td>[-, DIRECT, JUDGEMENTAL, DIRECT, DIRECT, DIRECT]</td>\n",
              "      <td>[[-], [IDEOLOGICAL-INEQUALITY, MISOGYNY-NON-SE...</td>\n",
              "      <td>DEV_EN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400182</th>\n",
              "      <td>en</td>\n",
              "      <td>@ultshunnie u gotta say some shit like ‚Äúi‚Äôll f...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_770, Annotator_771, Annotator_772, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
              "      <td>[DIRECT, REPORTED, DIRECT, DIRECT, JUDGEMENTAL...</td>\n",
              "      <td>[[OBJECTIFICATION, SEXUAL-VIOLENCE], [SEXUAL-V...</td>\n",
              "      <td>DEV_EN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "400178     en  1st day at the pool on a beautiful Sunday in N...   \n",
              "400179     en  ‚ÄúI like your outfit too except when i dress up...   \n",
              "400180     en  @KNasFanFic ü•∫üíñ same, though!!! the angst just ...   \n",
              "400181     en  @themaxburns @GOP Fuck that cunt. Tried to vot...   \n",
              "400182     en  @ultshunnie u gotta say some shit like ‚Äúi‚Äôll f...   \n",
              "\n",
              "         number_annotators                                         annotators  \\\n",
              "id_EXIST                                                                        \n",
              "400178                   6  [Annotator_764, Annotator_765, Annotator_766, ...   \n",
              "400179                   6  [Annotator_805, Annotator_426, Annotator_806, ...   \n",
              "400180                   6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
              "400181                   6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
              "400182                   6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
              "\n",
              "           gender_annotators                          age_annotators  \\\n",
              "id_EXIST                                                               \n",
              "400178    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "400179    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "400180    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "400181    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "400182    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "\n",
              "                            labels_task1  \\\n",
              "id_EXIST                                   \n",
              "400178          [NO, NO, NO, NO, NO, NO]   \n",
              "400179     [YES, YES, YES, YES, YES, NO]   \n",
              "400180          [NO, NO, NO, NO, NO, NO]   \n",
              "400181     [NO, YES, YES, YES, YES, YES]   \n",
              "400182    [YES, YES, YES, YES, YES, YES]   \n",
              "\n",
              "                                               labels_task2  \\\n",
              "id_EXIST                                                      \n",
              "400178                                   [-, -, -, -, -, -]   \n",
              "400179    [JUDGEMENTAL, DIRECT, REPORTED, DIRECT, REPORT...   \n",
              "400180                                   [-, -, -, -, -, -]   \n",
              "400181     [-, DIRECT, JUDGEMENTAL, DIRECT, DIRECT, DIRECT]   \n",
              "400182    [DIRECT, REPORTED, DIRECT, DIRECT, JUDGEMENTAL...   \n",
              "\n",
              "                                               labels_task3   split  \n",
              "id_EXIST                                                             \n",
              "400178                       [[-], [-], [-], [-], [-], [-]]  DEV_EN  \n",
              "400179    [[OBJECTIFICATION], [OBJECTIFICATION, MISOGYNY...  DEV_EN  \n",
              "400180                       [[-], [-], [-], [-], [-], [-]]  DEV_EN  \n",
              "400181    [[-], [IDEOLOGICAL-INEQUALITY, MISOGYNY-NON-SE...  DEV_EN  \n",
              "400182    [[OBJECTIFICATION, SEXUAL-VIOLENCE], [SEXUAL-V...  DEV_EN  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "f2EUuh1qLA1U",
        "outputId": "397a7b5e-53d9-4a0e-cab6-a0e17f6057d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>number_annotators</th>\n",
              "      <th>annotators</th>\n",
              "      <th>gender_annotators</th>\n",
              "      <th>age_annotators</th>\n",
              "      <th>labels_task1</th>\n",
              "      <th>labels_task2</th>\n",
              "      <th>labels_task3</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300001</th>\n",
              "      <td>es</td>\n",
              "      <td>@Fichinescu La comunidad gamer es un antro de ...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_726, Annotator_727, Annotator_357, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, YES, YES, NO, YES, NO]</td>\n",
              "      <td>[-, JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, -]</td>\n",
              "      <td>[[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [MISOGYN...</td>\n",
              "      <td>DEV_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300002</th>\n",
              "      <td>es</td>\n",
              "      <td>@anacaotica88 @MordorLivin No me acuerdo de lo...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
              "      <td>[JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...</td>\n",
              "      <td>[[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...</td>\n",
              "      <td>DEV_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300003</th>\n",
              "      <td>es</td>\n",
              "      <td>@cosmicJunkBot lo digo cada pocos dias y lo re...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_735, Annotator_736, Annotator_345, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
              "      <td>[-, -, -, -, -, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
              "      <td>DEV_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300004</th>\n",
              "      <td>es</td>\n",
              "      <td>Also mientras les decia eso la se√±alaba y deci...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
              "      <td>[-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...</td>\n",
              "      <td>[[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...</td>\n",
              "      <td>DEV_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300005</th>\n",
              "      <td>es</td>\n",
              "      <td>And all people killed,  attacked, harassed by ...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
              "      <td>[NO, YES, NO, NO, NO, NO]</td>\n",
              "      <td>[-, DIRECT, -, -, -, -]</td>\n",
              "      <td>[[-], [STEREOTYPING-DOMINANCE], [-], [-], [-],...</td>\n",
              "      <td>DEV_ES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "300001     es  @Fichinescu La comunidad gamer es un antro de ...   \n",
              "300002     es  @anacaotica88 @MordorLivin No me acuerdo de lo...   \n",
              "300003     es  @cosmicJunkBot lo digo cada pocos dias y lo re...   \n",
              "300004     es  Also mientras les decia eso la se√±alaba y deci...   \n",
              "300005     es  And all people killed,  attacked, harassed by ...   \n",
              "\n",
              "         number_annotators                                         annotators  \\\n",
              "id_EXIST                                                                        \n",
              "300001                   6  [Annotator_726, Annotator_727, Annotator_357, ...   \n",
              "300002                   6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
              "300003                   6  [Annotator_735, Annotator_736, Annotator_345, ...   \n",
              "300004                   6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
              "300005                   6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
              "\n",
              "           gender_annotators                          age_annotators  \\\n",
              "id_EXIST                                                               \n",
              "300001    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "300002    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "300003    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "300004    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "300005    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
              "\n",
              "                           labels_task1  \\\n",
              "id_EXIST                                  \n",
              "300001      [NO, YES, YES, NO, YES, NO]   \n",
              "300002    [YES, YES, NO, YES, YES, YES]   \n",
              "300003         [NO, NO, NO, NO, NO, NO]   \n",
              "300004    [NO, YES, YES, YES, YES, YES]   \n",
              "300005        [NO, YES, NO, NO, NO, NO]   \n",
              "\n",
              "                                               labels_task2  \\\n",
              "id_EXIST                                                      \n",
              "300001        [-, JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, -]   \n",
              "300002    [JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...   \n",
              "300003                                   [-, -, -, -, -, -]   \n",
              "300004    [-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...   \n",
              "300005                              [-, DIRECT, -, -, -, -]   \n",
              "\n",
              "                                               labels_task3   split  \n",
              "id_EXIST                                                             \n",
              "300001    [[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [MISOGYN...  DEV_ES  \n",
              "300002    [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  DEV_ES  \n",
              "300003                       [[-], [-], [-], [-], [-], [-]]  DEV_ES  \n",
              "300004    [[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...  DEV_ES  \n",
              "300005    [[-], [STEREOTYPING-DOMINANCE], [-], [-], [-],...  DEV_ES  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN8umkIkUfaE"
      },
      "source": [
        "### 3. Generate hard labels for Task 1 using majority voting and store them in a new dataframe column called hard_label_task1. Items without a clear majority will be removed from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "HA9xRI2IUk_2",
        "outputId": "c3739ce5-f9dc-4bbc-98cb-efa749806a6b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>number_annotators</th>\n",
              "      <th>annotators</th>\n",
              "      <th>gender_annotators</th>\n",
              "      <th>age_annotators</th>\n",
              "      <th>labels_task1</th>\n",
              "      <th>labels_task2</th>\n",
              "      <th>labels_task3</th>\n",
              "      <th>split</th>\n",
              "      <th>hard_label_task1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100001</th>\n",
              "      <td>es</td>\n",
              "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
              "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
              "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100002</th>\n",
              "      <td>es</td>\n",
              "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
              "      <td>[-, -, -, -, DIRECT, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100003</th>\n",
              "      <td>es</td>\n",
              "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
              "      <td>[-, -, -, -, -, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100004</th>\n",
              "      <td>es</td>\n",
              "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
              "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
              "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100005</th>\n",
              "      <td>es</td>\n",
              "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
              "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
              "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "100001     es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
              "100002     es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
              "100003     es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
              "100004     es  @Lunariita7 Un retraso social bastante lamenta...   \n",
              "100005     es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
              "\n",
              "         number_annotators                                         annotators  \\\n",
              "id_EXIST                                                                        \n",
              "100001                   6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
              "100002                   6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "100003                   6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "100004                   6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
              "100005                   6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
              "\n",
              "           gender_annotators                          age_annotators  \\\n",
              "id_EXIST                                                               \n",
              "100001    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100002    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100003    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100004    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100005    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "\n",
              "                           labels_task1  \\\n",
              "id_EXIST                                  \n",
              "100001    [YES, YES, NO, YES, YES, YES]   \n",
              "100002        [NO, NO, NO, NO, YES, NO]   \n",
              "100003         [NO, NO, NO, NO, NO, NO]   \n",
              "100004      [NO, NO, YES, NO, YES, YES]   \n",
              "100005     [YES, NO, YES, NO, YES, YES]   \n",
              "\n",
              "                                               labels_task2  \\\n",
              "id_EXIST                                                      \n",
              "100001    [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
              "100002                              [-, -, -, -, DIRECT, -]   \n",
              "100003                                   [-, -, -, -, -, -]   \n",
              "100004                [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
              "100005    [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
              "\n",
              "                                               labels_task3     split  \\\n",
              "id_EXIST                                                                \n",
              "100001    [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES   \n",
              "100002         [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES   \n",
              "100003                       [[-], [-], [-], [-], [-], [-]]  TRAIN_ES   \n",
              "100004    [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES   \n",
              "100005    [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES   \n",
              "\n",
              "         hard_label_task1  \n",
              "id_EXIST                   \n",
              "100001                YES  \n",
              "100002                 NO  \n",
              "100003                 NO  \n",
              "100004               None  \n",
              "100005                YES  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def majority_vote(labels):\n",
        "    \"\"\"\n",
        "    Determines the majority vote from a list of labels.\n",
        "    \"\"\"\n",
        "    yes_count = labels.count('YES')\n",
        "    no_count = labels.count('NO')\n",
        "    if yes_count > no_count:\n",
        "        return 'YES'\n",
        "    elif no_count > yes_count:\n",
        "        return 'NO'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply the majority_vote function to the 'labels_task1' column\n",
        "for df in [training_df, test_df, validation_df]:\n",
        "    df['hard_label_task1'] = df['labels_task1'].apply(majority_vote)\n",
        "\n",
        "# Display the result for the training DataFrame\n",
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X79adW4KUqlA",
        "outputId": "c5b5025f-9972-43be-f592-89795c81c87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Before dropping missing values ###\n",
            "Training DataFrame shape before dropping missing values: (6920, 11)\n",
            "Test DataFrame shape before dropping missing values: (312, 11)\n",
            "Validation DataFrame shape before dropping missing values: (726, 11)\n",
            "### After dropping missing values ###\n",
            "Training DataFrame shape after dropping missing values: (6064, 11)\n",
            "Test DataFrame shape after dropping missing values: (286, 11)\n",
            "Validation DataFrame shape after dropping missing values: (648, 11)\n"
          ]
        }
      ],
      "source": [
        "# Drop rows with missing values (for no majority vote case)\n",
        "print(\"### Before dropping missing values ###\")\n",
        "print(f\"Training DataFrame shape before dropping missing values: {training_df.shape}\")\n",
        "print(f\"Test DataFrame shape before dropping missing values: {test_df.shape}\")\n",
        "print(f\"Validation DataFrame shape before dropping missing values: {validation_df.shape}\")\n",
        "\n",
        "training_df = training_df.dropna()\n",
        "test_df = test_df.dropna()\n",
        "validation_df = validation_df.dropna()\n",
        "\n",
        "print(\"### After dropping missing values ###\")\n",
        "print(f\"Training DataFrame shape after dropping missing values: {training_df.shape}\")\n",
        "print(f\"Test DataFrame shape after dropping missing values: {test_df.shape}\")\n",
        "print(f\"Validation DataFrame shape after dropping missing values: {validation_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1VN-1QjVnB2"
      },
      "source": [
        "### 4. Keep only 'en' language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKlLRSblVn89",
        "outputId": "a8bbad70-fda6-4436-b95f-c05ae9f38fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Before filtering by language ###\n",
            "Training DataFrame shape before filtering by language: (6064, 11)\n",
            "Test DataFrame shape before filtering by language: (286, 11)\n",
            "Validation DataFrame shape before filtering by language: (648, 11)\n",
            "### After filtering by language ###\n",
            "Training DataFrame shape after filtering by language: (2870, 11)\n",
            "Test DataFrame shape after filtering by language: (286, 11)\n",
            "Validation DataFrame shape after filtering by language: (158, 11)\n"
          ]
        }
      ],
      "source": [
        "print(\"### Before filtering by language ###\")\n",
        "print(f\"Training DataFrame shape before filtering by language: {training_df.shape}\")\n",
        "print(f\"Test DataFrame shape before filtering by language: {test_df.shape}\")\n",
        "print(f\"Validation DataFrame shape before filtering by language: {validation_df.shape}\")\n",
        "\n",
        "training_df = training_df[training_df['lang'] == 'en']\n",
        "test_df = test_df[test_df['lang'] == 'en']\n",
        "validation_df = validation_df[validation_df['lang'] == 'en']\n",
        "\n",
        "print(\"### After filtering by language ###\")\n",
        "print(f\"Training DataFrame shape after filtering by language: {training_df.shape}\")\n",
        "print(f\"Test DataFrame shape after filtering by language: {test_df.shape}\")\n",
        "print(f\"Validation DataFrame shape after filtering by language: {validation_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBsax4wgU9zh"
      },
      "source": [
        "### 5. Keep only relevant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "OWlsFk1MU78y",
        "outputId": "0960af07-fcd0-41e3-c884-f0e4a2c3309a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>en</td>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>en</td>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200006</th>\n",
              "      <td>en</td>\n",
              "      <td>According to a customer I have plenty of time ...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200007</th>\n",
              "      <td>en</td>\n",
              "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200008</th>\n",
              "      <td>en</td>\n",
              "      <td>New to the shelves this week - looking forward...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "200002     en  Writing a uni essay in my local pub with a cof...   \n",
              "200003     en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
              "200006     en  According to a customer I have plenty of time ...   \n",
              "200007     en  So only 'blokes' drink beer? Sorry, but if you...   \n",
              "200008     en  New to the shelves this week - looking forward...   \n",
              "\n",
              "         hard_label_task1  \n",
              "id_EXIST                   \n",
              "200002                YES  \n",
              "200003                YES  \n",
              "200006                YES  \n",
              "200007                YES  \n",
              "200008                 NO  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df = training_df.drop(columns=['number_annotators',\t'annotators',\t'gender_annotators',\t'age_annotators',\t'labels_task1',\t'labels_task2',\t'labels_task3',\t'split'])\n",
        "test_df = test_df.drop(columns=['number_annotators',\t'annotators',\t'gender_annotators',\t'age_annotators',\t'labels_task1',\t'labels_task2',\t'labels_task3',\t'split'])\n",
        "validation_df = validation_df.drop(columns=['number_annotators',\t'annotators',\t'gender_annotators',\t'age_annotators',\t'labels_task1',\t'labels_task2',\t'labels_task3',\t'split'])\n",
        "\n",
        "training_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL8-pyk7VDs6"
      },
      "source": [
        "### 6. YES = 1, NO = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "9jJmxhlSVDYQ",
        "outputId": "b0886505-4c9b-4ce5-d754-1abfe58508b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>en</td>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>en</td>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200006</th>\n",
              "      <td>en</td>\n",
              "      <td>According to a customer I have plenty of time ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200007</th>\n",
              "      <td>en</td>\n",
              "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200008</th>\n",
              "      <td>en</td>\n",
              "      <td>New to the shelves this week - looking forward...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "200002     en  Writing a uni essay in my local pub with a cof...   \n",
              "200003     en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
              "200006     en  According to a customer I have plenty of time ...   \n",
              "200007     en  So only 'blokes' drink beer? Sorry, but if you...   \n",
              "200008     en  New to the shelves this week - looking forward...   \n",
              "\n",
              "          hard_label_task1  \n",
              "id_EXIST                    \n",
              "200002                   1  \n",
              "200003                   1  \n",
              "200006                   1  \n",
              "200007                   1  \n",
              "200008                   0  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for df in [training_df, test_df, validation_df]:\n",
        "    df['hard_label_task1'] = df['hard_label_task1'].map({'YES': 1, 'NO': 0})\n",
        "\n",
        "training_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Q-ED7XGFThSr"
      },
      "source": [
        "# [Task2 - 0.5 points] Data Cleaning\n",
        "In the context of tweets, we have noisy and informal data that often includes unnecessary elements like emojis, hashtags, mentions, and URLs. These elements may interfere with the text analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-YizhRa3ThSr"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "- **Remove emojis** from the tweets.\n",
        "- **Remove hashtags** (e.g., `#example`).\n",
        "- **Remove mentions** such as `@user`.\n",
        "- **Remove URLs** from the tweets.\n",
        "- **Remove special characters and symbols**.\n",
        "- **Remove specific quote characters** (e.g., curly quotes).\n",
        "- **Perform lemmatization** to reduce words to their base form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8omiF2i3cWH"
      },
      "source": [
        "### 0. Adjust the stop symbols (this was not requested but I added it anyway because it was done in class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KVCYvkd3k7U",
        "outputId": "24ca363b-9635-4039-d8bd-42129780699c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qsRa-KctJlS"
      },
      "source": [
        "### 1. Remove emojis\n",
        "\n",
        "In order to complete this task we need to import the regex. Of course this is taken from the internet because where the fuck was I able to get the fucking regex of the emojis, I don't know shit about this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e5y8_gVi4k93"
      },
      "outputs": [],
      "source": [
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    \"\"\"\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "A_aLUigv6n99",
        "outputId": "71d3691f-b11a-4984-c7ad-0123a0472753"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>400178</th>\n",
              "      <td>en</td>\n",
              "      <td>1st day at the pool on a beautiful Sunday in N...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400179</th>\n",
              "      <td>en</td>\n",
              "      <td>‚ÄúI like your outfit too except when i dress up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400180</th>\n",
              "      <td>en</td>\n",
              "      <td>@KNasFanFic ü•∫üíñ same, though!!! the angst just ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400181</th>\n",
              "      <td>en</td>\n",
              "      <td>@themaxburns @GOP Fuck that cunt. Tried to vot...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400182</th>\n",
              "      <td>en</td>\n",
              "      <td>@ultshunnie u gotta say some shit like ‚Äúi‚Äôll f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "400178     en  1st day at the pool on a beautiful Sunday in N...   \n",
              "400179     en  ‚ÄúI like your outfit too except when i dress up...   \n",
              "400180     en  @KNasFanFic ü•∫üíñ same, though!!! the angst just ...   \n",
              "400181     en  @themaxburns @GOP Fuck that cunt. Tried to vot...   \n",
              "400182     en  @ultshunnie u gotta say some shit like ‚Äúi‚Äôll f...   \n",
              "\n",
              "          hard_label_task1  \n",
              "id_EXIST                    \n",
              "400178                   0  \n",
              "400179                   1  \n",
              "400180                   0  \n",
              "400181                   1  \n",
              "400182                   1  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3cPygsyAtMdc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # By encoding in 'ascii', emojis are removed\n",
        "    return text.encode('ascii', 'ignore').decode('ascii')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sObTCR8a4v0q"
      },
      "source": [
        "### 2. Remove hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fTdapHQu4x7D"
      },
      "outputs": [],
      "source": [
        "def remove_hashtags(text):\n",
        "    # Regex pattern to match hashtags\n",
        "    hashtag_pattern = r'#\\w+'  # Matches words starting with # followed by any alphanumeric characters or underscores\n",
        "\n",
        "    # Substitute hashtags with an empty string\n",
        "    return re.sub(hashtag_pattern, '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtc48h9R5B7b"
      },
      "source": [
        "### 3. Remove mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YBpp2w-95Dqr"
      },
      "outputs": [],
      "source": [
        "def remove_mentions(text):\n",
        "    # Regex pattern to match Twitter mentions (e.g., @username)\n",
        "    mention_pattern = r'@\\w+'  # Matches @ followed by any alphanumeric characters or underscores\n",
        "\n",
        "    # Substitute mentions with an empty string\n",
        "    return re.sub(mention_pattern, '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss5aF1ha6T23"
      },
      "source": [
        "### 4. Remove urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qU5s-USe6V24"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    # Regex pattern to match URLs (http://, https://, ftp://, etc.)\n",
        "    url_pattern = r'http[s]?://\\S+'  # Matches URLs starting with http:// or https:// and followed by non-whitespace characters\n",
        "\n",
        "    # Substitute URLs with an empty string\n",
        "    return re.sub(url_pattern, '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdV1p9G-4Yo-"
      },
      "source": [
        "### 5. Remove special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lfWh5nwr4bnK"
      },
      "outputs": [],
      "source": [
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis, with spacing character\n",
        "    \"\"\"\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqmZZAvI6fC2"
      },
      "source": [
        "### 6. Remove quotations characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WEJxw_jc6pJG"
      },
      "outputs": [],
      "source": [
        "def remove_quotations(text):\n",
        "    # Regex pattern to match single and double quotes\n",
        "    quote_pattern = r\"[\\\"'‚Äò‚Äô‚Äú‚Äù]\"  # Matches either single quotes (') or double quotes (\")\n",
        "\n",
        "    # Substitute quotation marks with an empty string\n",
        "    return re.sub(quote_pattern, '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I5poHHG6uBT"
      },
      "source": [
        "Before going ahead with lemmatization I want to perform this to the whole column containing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Eb4eIfyU6zhy"
      },
      "outputs": [],
      "source": [
        "# typing\n",
        "from typing import List, Callable, Dict\n",
        "from collections import OrderedDict\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          lower,\n",
        "                          remove_emojis,\n",
        "                          remove_hashtags,\n",
        "                          remove_mentions,\n",
        "                          remove_urls,\n",
        "                          replace_special_characters,\n",
        "                          remove_quotations\n",
        "                          ]\n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHz1A9Z-7Xpp",
        "outputId": "4a83fc1a-790c-4c14-887f-2829f91b301d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-processing text...\n",
            "\n",
            "[Debug train] Before:\n",
            "@LibertyAnders I get it.. kind of. 80% of women are going after about 20% of men and ignoring the rest according to statistics of 3 dating apps. so that‚Äôs a pain point for some men, but Is hating women really a main component to being an incel? And what‚Äôs the difference between MGTOW and Incel?\n",
            "[Debug test] Before:\n",
            "@HTFCirno2000 I have an old receipt printer thing but I have no idea how to use it. I would like to fuck with it like you did. https://t.co/UpXFySfBwB\n",
            "[Debug validation] Before:\n",
            "@motahedoon Alrubaye bint moawad Good luck üíúüíúüíúüíúü§çü§ç#motahedoon_challeng\n",
            "\n",
            "[Debug train] After:\n",
            " i get it.. kind of. 80% of women are going after about 20% of men and ignoring the rest according to statistics of 3 dating apps. so thats a pain point for some men  but is hating women really a main component to being an incel? and whats the difference between mgtow and incel?\n",
            "[Debug test] After:\n",
            " i have an old receipt printer thing but i have no idea how to use it. i would like to fuck with it like you did. \n",
            "[Debug validation] After:\n",
            " alrubaye bint moawad good luck \n",
            "\n",
            "Pre-processing completed!\n"
          ]
        }
      ],
      "source": [
        "print('Pre-processing text...')\n",
        "\n",
        "print()\n",
        "print(f'[Debug train] Before:\\n{training_df.tweet.values[50]}')\n",
        "print(f'[Debug test] Before:\\n{test_df.tweet.values[50]}')\n",
        "print(f'[Debug validation] Before:\\n{validation_df.tweet.values[50]}')\n",
        "print()\n",
        "\n",
        "# Replace each sentence with its pre-processed version\n",
        "training_df['tweet'] = training_df['tweet'].apply(lambda txt: text_prepare(txt))\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda txt: text_prepare(txt))\n",
        "validation_df['tweet'] = validation_df['tweet'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "\n",
        "print(f'[Debug train] After:\\n{training_df.tweet.values[50]}')\n",
        "print(f'[Debug test] After:\\n{test_df.tweet.values[50]}')\n",
        "print(f'[Debug validation] After:\\n{validation_df.tweet.values[50]}')\n",
        "print()\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8pMrdK86qk-"
      },
      "source": [
        "### 7. Perform lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pniRR2yK8b74",
        "outputId": "c3c26c66-c65c-4ad7-8449-8d56189c0c6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\WIN11\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\WIN11\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\WIN11\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\WIN11\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#creating lemmatizer object\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "lemmatizer = nltk.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgEtRZzw8gAM",
        "outputId": "5af20242-5196-408e-e64b-bb28d6b2db1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2870/2870 [00:02<00:00, 1163.83it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286/286 [00:00<00:00, 2448.95it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [00:00<00:00, 3231.36it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_key(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "def lem_text(text: str):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "    return \" \".join(words)\n",
        "\n",
        "training_df['tweet'] = [lem_text(text) for text in tqdm(training_df['tweet'], leave=True, position=0)]\n",
        "test_df['tweet'] = [lem_text(text) for text in tqdm(test_df['tweet'], leave=True, position=0)]\n",
        "validation_df['tweet'] = [lem_text(text) for text in tqdm(validation_df['tweet'], leave=True, position=0)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RwczWZN885vG"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>en</td>\n",
              "      <td>write a uni essay in my local pub with a coffe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>en</td>\n",
              "      <td>it be 2021 not 1921. i dont appreciate that on...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200006</th>\n",
              "      <td>en</td>\n",
              "      <td>accord to a customer i have plenty of time to ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200007</th>\n",
              "      <td>en</td>\n",
              "      <td>so only blokes drink beer? sorry but if you ar...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200008</th>\n",
              "      <td>en</td>\n",
              "      <td>new to the shelf this week - look forward to r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "200002     en  write a uni essay in my local pub with a coffe...   \n",
              "200003     en  it be 2021 not 1921. i dont appreciate that on...   \n",
              "200006     en  accord to a customer i have plenty of time to ...   \n",
              "200007     en  so only blokes drink beer? sorry but if you ar...   \n",
              "200008     en  new to the shelf this week - look forward to r...   \n",
              "\n",
              "          hard_label_task1  \n",
              "id_EXIST                    \n",
              "200002                   1  \n",
              "200003                   1  \n",
              "200006                   1  \n",
              "200007                   1  \n",
              "200008                   0  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fkGDxYXV9ISg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Debug] After:\n",
            "i get it.. kind of. 80% of woman be go after about 20% of men and ignore the rest accord to statistic of 3 date apps. so thats a pain point for some men but be hat woman really a main component to be an incel? and whats the difference between mgtow and incel?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'[Debug] After:\\n{training_df.tweet.values[50]}')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3KylLHNl0bE"
      },
      "source": [
        "# [Task 3 - 0.5 points] Text Encoding\n",
        "To train a neural sexism classifier, you first need to encode text into numerical format.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr1lTHUVOXff"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6NNMEjWOZQr"
      },
      "source": [
        "### Note : What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe **must** be added to the vocabulary.\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **special token** (e.g., [UNK]) and a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90UztlGUObXk"
      },
      "source": [
        "### More about OOV\n",
        "\n",
        "For a given token:\n",
        "\n",
        "* **If in train set**: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\n",
        "* **If in val/test set**: assign special token if not in vocabulary and assign custom embedding.\n",
        "\n",
        "Your vocabulary **should**:\n",
        "\n",
        "* Contain all tokens in train set; or\n",
        "* Union of tokens in train set and in GloVe $\\rightarrow$ we make use of existing knowledge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nAgr9-dS6n9_"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "import gzip\n",
        "\n",
        "def load_embedding_model(model_type: str,\n",
        "                         return_path: bool,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"\"\n",
        "    if model_type.strip().lower() == 'glove-twitter':\n",
        "        download_path = \"glove-twitter-{}\".format(embedding_dimension)\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove-wiki-gigaword':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: glove-wiki-gigaword, glove-twitter\")\n",
        "\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path,return_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"glove-twitter: 25, 50, 100, 200\")\n",
        "        print(\"glove-wiki-gigaword: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "import gzip\n",
        "\n",
        "def from_txt_to_dictionary(glove_file_path):\n",
        "    embeddings = {}\n",
        "\n",
        "    with gzip.open(glove_file_path, 'rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Split each line into word and its embedding\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = list(map(float, values[1:]))\n",
        "            embeddings[word] = vector\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zflQ2UAfZ8Lb"
      },
      "source": [
        "Now we need to define the functions to build the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bjAzyQhnaAKs"
      },
      "outputs": [],
      "source": [
        "# Initialize vocabulary with GloVe and custom embeddings\n",
        "def build_vocabulary(train_tokens, glove_embeddings, embedding_dim=50):\n",
        "    vocabulary = {}\n",
        "    for token in set(train_tokens):\n",
        "        if token in glove_embeddings:\n",
        "            vocabulary[token] = glove_embeddings[token]\n",
        "        else:\n",
        "            # Assign random vector for OOV tokens in the train set\n",
        "            vocabulary[token] = np.random.uniform(-0.5, 0.5, embedding_dim)\n",
        "    return vocabulary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oAfbGhnaGny"
      },
      "source": [
        "Then the embeddings from the text and the OOV case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QYWixyezaJhm"
      },
      "outputs": [],
      "source": [
        "# Define the [UNK] embedding as the average of all embeddings\n",
        "def define_unk_embedding(vocabulary):\n",
        "    return np.mean(list(vocabulary.values()), axis=0)\n",
        "\n",
        "# Convert tokens in text to embeddings\n",
        "def text_to_embeddings(text, vocabulary, unk_embedding):\n",
        "    embeddings = []\n",
        "    for token in text.split():\n",
        "        token = token.lower()\n",
        "        if token in vocabulary:\n",
        "            embeddings.append(vocabulary[token])\n",
        "        else:\n",
        "            embeddings.append(unk_embedding)  # Use [UNK] embedding for unknown tokens\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HWUe1cE-WoZU"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 200\n",
        "glove_file_path=load_embedding_model('glove-twitter', return_path=True, embedding_dimension=embedding_dim)\n",
        "glove_embeddings=from_txt_to_dictionary(glove_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-qqNvkq7aVZI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tokenize and build vocabulary using only training tokens\n",
        "train_tokens = [word.lower() for tweet in training_df['tweet'] for word in tweet.split()]\n",
        "vocabulary = build_vocabulary(train_tokens, glove_embeddings, embedding_dim)\n",
        "\n",
        "# Step 3: Define [UNK] embedding\n",
        "unk_embedding = define_unk_embedding(vocabulary)\n",
        "\n",
        "# Step 4: Embed the 'tweet' column in each dataframe\n",
        "def embed_tweets(df, vocabulary, unk_embedding):\n",
        "    df['tweet_embedding'] = df['tweet'].apply(lambda tweet: text_to_embeddings(tweet, vocabulary, unk_embedding))\n",
        "    return df\n",
        "\n",
        "# Apply to each dataframe\n",
        "training_df = embed_tweets(training_df, vocabulary, unk_embedding)\n",
        "test_df = embed_tweets(test_df, vocabulary, unk_embedding)\n",
        "validation_df = embed_tweets(validation_df, vocabulary, unk_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "rprLyOOHWtPN"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "      <th>tweet_embedding</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>en</td>\n",
              "      <td>write a uni essay in my local pub with a coffe...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[0.23606, 0.46571, 0.0091574, -0.022217, -0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>en</td>\n",
              "      <td>it be 2021 not 1921. i dont appreciate that on...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-0.20397, 0.45338, -0.1684, 0.14302, -0.0948...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200006</th>\n",
              "      <td>en</td>\n",
              "      <td>accord to a customer i have plenty of time to ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-0.078828, -0.13376, -0.50952, 0.46567, 0.04...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200007</th>\n",
              "      <td>en</td>\n",
              "      <td>so only blokes drink beer? sorry but if you ar...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[0.12018, 0.038966, 0.027403, 0.16089, 0.1336...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200008</th>\n",
              "      <td>en</td>\n",
              "      <td>new to the shelf this week - look forward to r...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[0.27554, 0.15505, -0.39506, 0.35, 0.018967, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "200002     en  write a uni essay in my local pub with a coffe...   \n",
              "200003     en  it be 2021 not 1921. i dont appreciate that on...   \n",
              "200006     en  accord to a customer i have plenty of time to ...   \n",
              "200007     en  so only blokes drink beer? sorry but if you ar...   \n",
              "200008     en  new to the shelf this week - look forward to r...   \n",
              "\n",
              "          hard_label_task1                                    tweet_embedding  \n",
              "id_EXIST                                                                       \n",
              "200002                   1  [[0.23606, 0.46571, 0.0091574, -0.022217, -0.4...  \n",
              "200003                   1  [[-0.20397, 0.45338, -0.1684, 0.14302, -0.0948...  \n",
              "200006                   1  [[-0.078828, -0.13376, -0.50952, 0.46567, 0.04...  \n",
              "200007                   1  [[0.12018, 0.038966, 0.027403, 0.16089, 0.1336...  \n",
              "200008                   0  [[0.27554, 0.15505, -0.39506, 0.35, 0.018967, ...  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IysrEkS1WttI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "      <th>tweet_embedding</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>400001</th>\n",
              "      <td>en</td>\n",
              "      <td>you should smile more love. just pretend youre...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[0.1964, 0.67153, 0.0062976, 0.25359, -0.4209...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400002</th>\n",
              "      <td>en</td>\n",
              "      <td>she be right but the push be all in the opposi...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-0.18295, 0.28614, 0.11604, 0.36726, 0.17055...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400003</th>\n",
              "      <td>en</td>\n",
              "      <td>some man move my suitcase in the overhead lugg...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[0.00096193, -0.1576, -0.15985, 0.037884, 0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400004</th>\n",
              "      <td>en</td>\n",
              "      <td>lol gamergate the go to boogieman maybe if the...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[0.3043, -0.047127, 0.0067446, -0.070278, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400005</th>\n",
              "      <td>en</td>\n",
              "      <td>to me this have the same negativity a gamergat...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[0.61774, 0.21046, 0.52698, 0.20467, 0.257, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "400001     en  you should smile more love. just pretend youre...   \n",
              "400002     en  she be right but the push be all in the opposi...   \n",
              "400003     en  some man move my suitcase in the overhead lugg...   \n",
              "400004     en  lol gamergate the go to boogieman maybe if the...   \n",
              "400005     en  to me this have the same negativity a gamergat...   \n",
              "\n",
              "          hard_label_task1                                    tweet_embedding  \n",
              "id_EXIST                                                                       \n",
              "400001                   0  [[0.1964, 0.67153, 0.0062976, 0.25359, -0.4209...  \n",
              "400002                   1  [[-0.18295, 0.28614, 0.11604, 0.36726, 0.17055...  \n",
              "400003                   1  [[0.00096193, -0.1576, -0.15985, 0.037884, 0.1...  \n",
              "400004                   0  [[0.3043, -0.047127, 0.0067446, -0.070278, -0....  \n",
              "400005                   0  [[0.61774, 0.21046, 0.52698, 0.20467, 0.257, -...  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "LFJ-_m_TbJRa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hard_label_task1</th>\n",
              "      <th>tweet_embedding</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_EXIST</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>400178</th>\n",
              "      <td>en</td>\n",
              "      <td>1st day at the pool on a beautiful sunday in n...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[0.05022178245947084, -0.07373889115771826, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400179</th>\n",
              "      <td>en</td>\n",
              "      <td>i like your outfit too except when i dress up ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[0.056404, 0.49536, 0.18439, 0.054535, -0.517...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400180</th>\n",
              "      <td>en</td>\n",
              "      <td>same though!!! the angst just come and goes. l...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[0.48162, -0.080498, 0.089994, 0.794, 0.15886...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400181</th>\n",
              "      <td>en</td>\n",
              "      <td>fuck that cunt. try to vote her out multiple time</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-0.20878, 0.16733, -0.0078498, 0.11349, -0.6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400182</th>\n",
              "      <td>en</td>\n",
              "      <td>u gotta say some shit like ill fuck that cunt ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[0.38137, 0.36436, 0.25135, 0.13976, 0.026781...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lang                                              tweet  \\\n",
              "id_EXIST                                                           \n",
              "400178     en  1st day at the pool on a beautiful sunday in n...   \n",
              "400179     en  i like your outfit too except when i dress up ...   \n",
              "400180     en  same though!!! the angst just come and goes. l...   \n",
              "400181     en  fuck that cunt. try to vote her out multiple time   \n",
              "400182     en  u gotta say some shit like ill fuck that cunt ...   \n",
              "\n",
              "          hard_label_task1                                    tweet_embedding  \n",
              "id_EXIST                                                                       \n",
              "400178                   0  [[0.05022178245947084, -0.07373889115771826, -...  \n",
              "400179                   1  [[0.056404, 0.49536, 0.18439, 0.054535, -0.517...  \n",
              "400180                   0  [[0.48162, -0.080498, 0.089994, 0.794, 0.15886...  \n",
              "400181                   1  [[-0.20878, 0.16733, -0.0078498, 0.11349, -0.6...  \n",
              "400182                   1  [[0.38137, 0.36436, 0.25135, 0.13976, 0.026781...  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JLnuLGHGAUT"
      },
      "source": [
        "# [Task 4 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your sexism classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQFI9J-JOfXD"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jALc_qYGS2E"
      },
      "source": [
        "### Token to embedding mapping\n",
        "\n",
        "You can follow two approaches for encoding tokens in your classifier.\n",
        "\n",
        "### Work directly with embeddings\n",
        "\n",
        "- Compute the embedding of each input token\n",
        "- Feed the mini-batches of shape (batch_size, # tokens, embedding_dim) to your model\n",
        "\n",
        "### Work with Embedding layer\n",
        "\n",
        "- Encode input tokens to token ids\n",
        "- Define a Embedding layer as the first layer of your model\n",
        "- Compute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\n",
        "- Initialize the Embedding layer with the computed embedding matrix\n",
        "- You are **free** to set the Embedding layer trainable or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zGUXH2_8gZlE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Z44PckrNGfTv"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocabulary)\n",
        "embedding_dimension = embedding_dim\n",
        "embedding_matrix = embedding_matrix = np.array(list(vocabulary.values()))\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                      output_dim=embedding_dimension,\n",
        "                                      weights=[embedding_matrix],\n",
        "                                      mask_zero=True,                   # automatically masks padding tokens\n",
        "                                      name='encoder_embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEQTPu6eGgGv"
      },
      "source": [
        "### Padding\n",
        "\n",
        "Pay attention to padding tokens!\n",
        "\n",
        "Your model **should not** be penalized on those tokens.\n",
        "\n",
        "#### How to?\n",
        "\n",
        "There are two main ways.\n",
        "\n",
        "However, their implementation depends on the neural library you are using.\n",
        "\n",
        "- Embedding layer\n",
        "- Custom loss to compute average cross-entropy on non-padding tokens only\n",
        "\n",
        "**Note**: This is a **recommendation**, but we **do not penalize** for missing workarounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KJf7rKTdqZeC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Ensure that each tweet embedding is padded to a consistent length\n",
        "sequence_length = 100  # Define based on your needs or average embedding length in data\n",
        "\n",
        "# Apply padding directly on the tweet_embedding column\n",
        "X_train = pad_sequences(training_df['tweet_embedding'].tolist(), maxlen=sequence_length, dtype='float32', padding='post', truncating='post')\n",
        "X_val = pad_sequences(validation_df['tweet_embedding'].tolist(), maxlen=sequence_length, dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = training_df['hard_label_task1'].values\n",
        "y_val = validation_df['hard_label_task1'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "JR3dt9-nqwmM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, TimeDistributed\n",
        "\n",
        "'''\n",
        "REMEMBER: YOU SHOULD WRAP THE DENSE LAYER WITH TIMEDISTRIBUTED BUT I'M NOT FIGURING OUT HOW TO FIX THE SHAPE PROBLEMS DHN\n",
        "'''\n",
        "def create_baseline_model(input_shape, lstm_units=128):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=False), input_shape=input_shape))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OWtAh8ZKqxJK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "REMEMBER: YOU SHOULD WRAP THE DENSE LAYER WITH TIMEDISTRIBUTED BUT I'M NOT FIGURING OUT HOW TO FIX THE SHAPE PROBLEMS DHN\n",
        "'''\n",
        "\n",
        "def create_model_1(input_shape, lstm_units=128):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True), input_shape=input_shape))  # First LSTM layer\n",
        "    model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=False)))  # Second LSTM layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjBgdiRG3wD"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline and Model 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWPK4umGOjtT"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance.\n",
        "* Evaluate your models using macro F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wSShowXXpMUT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# Custom Callback to compute F1-score\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, val_data):\n",
        "        self.val_data = val_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_x, val_y = self.val_data\n",
        "        val_predictions = (self.model.predict(val_x) > 0.5).astype(int)  # Binarize predictions\n",
        "        f1 = f1_score(val_y, val_predictions, average=\"macro\")\n",
        "        print(f\"Epoch {epoch + 1}: Macro F1-Score = {f1:.4f}\")\n",
        "        logs['val_f1'] = f1  # Add F1 to logs for history tracking\n",
        "\n",
        "# Early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "q53Q9CHdr8Sx"
      },
      "outputs": [],
      "source": [
        "input_shape = (sequence_length, X_train.shape[2])  # Shape of each input sample: (sequence_length, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir9nPtDptuVr"
      },
      "source": [
        "#### Training with random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6crRCWn0tlxy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seeds(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0NWn8EjhJNFy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1hqamc1ztp0E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with seed 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Epoch 1: Macro F1-Score = 0.7511\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Epoch 2: Macro F1-Score = 0.7530\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Epoch 3: Macro F1-Score = 0.8109\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 4: Macro F1-Score = 0.7864\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 5: Macro F1-Score = 0.8223\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 6: Macro F1-Score = 0.8142\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Epoch 7: Macro F1-Score = 0.7727\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 8: Macro F1-Score = 0.7821\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Epoch 9: Macro F1-Score = 0.7985\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Epoch 10: Macro F1-Score = 0.8045\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Seed 42 - Macro F1-Score: 0.8045\n",
            "Training with seed 123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Epoch 1: Macro F1-Score = 0.7522\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 2: Macro F1-Score = 0.7727\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Epoch 3: Macro F1-Score = 0.7589\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 4: Macro F1-Score = 0.7761\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Epoch 5: Macro F1-Score = 0.7911\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 6: Macro F1-Score = 0.7925\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 7: Macro F1-Score = 0.8192\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 8: Macro F1-Score = 0.7985\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 9: Macro F1-Score = 0.7937\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 10: Macro F1-Score = 0.7511\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Seed 123 - Macro F1-Score: 0.7511\n",
            "Training with seed 789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Epoch 1: Macro F1-Score = 0.7629\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 2: Macro F1-Score = 0.7924\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Epoch 3: Macro F1-Score = 0.7874\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 4: Macro F1-Score = 0.7995\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 5: Macro F1-Score = 0.8152\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 6: Macro F1-Score = 0.8253\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 7: Macro F1-Score = 0.7865\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 8: Macro F1-Score = 0.7908\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 9: Macro F1-Score = 0.7667\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 10: Macro F1-Score = 0.7656\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Seed 789 - Macro F1-Score: 0.7656\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 123, 789]  # Example seeds\n",
        "baseline_f1_scores = []\n",
        "model_1_f1_scores = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"Training with seed {seed}\")\n",
        "    set_seeds(seed)\n",
        "\n",
        "    # Create and compile the baseline model\n",
        "    baseline_model = create_baseline_model(input_shape=input_shape, lstm_units=128)\n",
        "    baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    f1_callback = F1ScoreCallback(val_data=(X_val, y_val))\n",
        "    # Train the model\n",
        "    baseline_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[f1_callback], verbose=0)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_predictions = (baseline_model.predict(X_val) > 0.5).astype(int)\n",
        "    macro_f1 = f1_score(y_val, val_predictions, average='macro')\n",
        "    baseline_f1_scores.append(macro_f1)\n",
        "    print(f\"Seed {seed} - Macro F1-Score: {macro_f1:.4f}\")\n",
        "\n",
        "    #print(classification_report(y_val, val_predictions, target_names=['Not Sexist', 'Sexist']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Jv-HjBByBYVZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with seed 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step\n",
            "Epoch 1: Macro F1-Score = 0.7393\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 2: Macro F1-Score = 0.7776\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 3: Macro F1-Score = 0.8040\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 4: Macro F1-Score = 0.8081\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 5: Macro F1-Score = 0.7839\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 6: Macro F1-Score = 0.8203\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 7: Macro F1-Score = 0.7709\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 8: Macro F1-Score = 0.8131\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 9: Macro F1-Score = 0.8064\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 10: Macro F1-Score = 0.8179\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Seed 42 - Macro F1-Score: 0.7656\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Not Sexist       0.76      0.89      0.82        90\n",
            "      Sexist       0.81      0.63      0.71        68\n",
            "\n",
            "    accuracy                           0.78       158\n",
            "   macro avg       0.79      0.76      0.77       158\n",
            "weighted avg       0.78      0.78      0.77       158\n",
            "\n",
            "Training with seed 123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step\n",
            "Epoch 1: Macro F1-Score = 0.7334\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 2: Macro F1-Score = 0.7161\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Epoch 3: Macro F1-Score = 0.8142\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 4: Macro F1-Score = 0.7361\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 5: Macro F1-Score = 0.7008\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 6: Macro F1-Score = 0.7744\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 7: Macro F1-Score = 0.7709\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 8: Macro F1-Score = 0.7684\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 9: Macro F1-Score = 0.7582\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 10: Macro F1-Score = 0.7696\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Seed 123 - Macro F1-Score: 0.7656\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Not Sexist       0.76      0.89      0.82        90\n",
            "      Sexist       0.81      0.63      0.71        68\n",
            "\n",
            "    accuracy                           0.78       158\n",
            "   macro avg       0.79      0.76      0.77       158\n",
            "weighted avg       0.78      0.78      0.77       158\n",
            "\n",
            "Training with seed 789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step\n",
            "Epoch 1: Macro F1-Score = 0.7457\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 2: Macro F1-Score = 0.7797\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 3: Macro F1-Score = 0.8162\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 4: Macro F1-Score = 0.8377\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 5: Macro F1-Score = 0.8101\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 6: Macro F1-Score = 0.7816\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 7: Macro F1-Score = 0.8131\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 8: Macro F1-Score = 0.8395\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 9: Macro F1-Score = 0.8247\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Epoch 10: Macro F1-Score = 0.7727\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Seed 789 - Macro F1-Score: 0.7656\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Not Sexist       0.76      0.89      0.82        90\n",
            "      Sexist       0.81      0.63      0.71        68\n",
            "\n",
            "    accuracy                           0.78       158\n",
            "   macro avg       0.79      0.76      0.77       158\n",
            "weighted avg       0.78      0.78      0.77       158\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for seed in seeds:\n",
        "    print(f\"Training with seed {seed}\")\n",
        "    set_seeds(seed)\n",
        "\n",
        "    # Create and compile the baseline model\n",
        "    model_1 = create_model_1(input_shape=input_shape, lstm_units=128)\n",
        "    model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    f1_callback = F1ScoreCallback(val_data=(X_val, y_val))\n",
        "    # Train the model\n",
        "    model_1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks = [f1_callback], verbose=0)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_predictions = (baseline_model.predict(X_val) > 0.5).astype(int)\n",
        "    macro_f1 = f1_score(y_val, val_predictions, average='macro')\n",
        "    model_1_f1_scores.append(macro_f1)\n",
        "    print(f\"Seed {seed} - Macro F1-Score: {macro_f1:.4f}\")\n",
        "\n",
        "    print(classification_report(y_val, val_predictions, target_names=['Not Sexist', 'Sexist']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaEfko-KuEWd"
      },
      "source": [
        "#### Pick the best performing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DA8X4iioCV99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.804547199590496, 0.7510504201680672, 0.7656283110828566, 0.7656283110828566, 0.7656283110828566, 0.7656283110828566]\n"
          ]
        }
      ],
      "source": [
        "f1_scores = baseline_f1_scores + model_1_f1_scores\n",
        "print(f1_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eu3BjGKzCWbh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model: Baseline with Best Seed: 42 with Macro F1-Score: 0.8045\n"
          ]
        }
      ],
      "source": [
        "best_seed = seeds[np.argmax(f1_scores)]\n",
        "best_f1_baseline = max(baseline_f1_scores)\n",
        "best_f1_model_1 = max(model_1_f1_scores)\n",
        "best_f1 = max(best_f1_baseline, best_f1_model_1)\n",
        "best_model = \"Baseline\" if best_f1 == best_f1_baseline else \"Model 1\"\n",
        "print(f\"Best Model: {best_model} with Best Seed: {best_seed} with Macro F1-Score: {best_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIgpzixJuSBX"
      },
      "source": [
        "#### Evaluate best performing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1mvoQW5luUgZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6476 - loss: 0.6310 - val_accuracy: 0.7722 - val_loss: 0.5060\n",
            "Epoch 2/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7707 - loss: 0.4927 - val_accuracy: 0.7722 - val_loss: 0.5137\n",
            "Epoch 3/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8012 - loss: 0.4467 - val_accuracy: 0.8165 - val_loss: 0.4559\n",
            "Epoch 4/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8203 - loss: 0.4130 - val_accuracy: 0.7975 - val_loss: 0.4829\n",
            "Epoch 5/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8434 - loss: 0.3668 - val_accuracy: 0.8291 - val_loss: 0.4758\n",
            "Epoch 6/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8711 - loss: 0.3253 - val_accuracy: 0.8228 - val_loss: 0.5170\n",
            "Epoch 7/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8758 - loss: 0.3118 - val_accuracy: 0.7911 - val_loss: 0.5602\n",
            "Epoch 8/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9070 - loss: 0.2413 - val_accuracy: 0.7975 - val_loss: 0.6211\n",
            "Epoch 9/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9285 - loss: 0.1935 - val_accuracy: 0.8101 - val_loss: 0.5026\n",
            "Epoch 10/10\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9244 - loss: 0.2128 - val_accuracy: 0.8165 - val_loss: 0.5398\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x189bca17490>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seeds(best_seed)\n",
        "if best_model == 'Baseline':\n",
        "    best_model = create_baseline_model(input_shape=input_shape, lstm_units=128)\n",
        "else:\n",
        "    best_model = create_model_1(input_shape=input_shape, lstm_units=128)\n",
        "\n",
        "best_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Retrain the best model\n",
        "best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nanNZQg-SG36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Final Model (Seed 42) - Macro F1-Score: 0.8045\n"
          ]
        }
      ],
      "source": [
        "# Final Evaluation\n",
        "final_val_predictions = (best_model.predict(X_val) > 0.5).astype(int)\n",
        "final_macro_f1 = f1_score(y_val, final_val_predictions, average='macro')\n",
        "print(f\"Final Model (Seed {best_seed}) - Macro F1-Score: {final_macro_f1:.4f}\")\n",
        "\n",
        "best_cr = classification_report(y_val, final_val_predictions, target_names=['Not Sexist', 'Sexist'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSy9sPwYHUoD"
      },
      "source": [
        "# [Task 6 - 1.0 points] Transformers\n",
        "\n",
        "In this section, you will use a transformer model specifically trained for hate speech detection, namely [Twitter-roBERTa-base for Hate Speech Detection](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "m59tkUJKThSs"
      },
      "source": [
        "### Relevant Material\n",
        "- Tutorial 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1JLBffAaThSs"
      },
      "source": [
        "### Instructions\n",
        "1. **Load the Tokenizer and Model**\n",
        "\n",
        "2. **Preprocess the Dataset**:\n",
        "   You will need to preprocess your dataset to prepare it for input into the model. Tokenize your text data using the appropriate tokenizer and ensure it is formatted correctly.\n",
        "\n",
        "   **Note**: You have to use the plain text of the dataset and not the version that you tokenized before, as you need to tokenize the cleaned text obtained after the initial cleaning process.\n",
        "\n",
        "3. **Train the Model**:\n",
        "   Use the `Trainer` to train the model on your training data.\n",
        "\n",
        "4. **Evaluate the Model on the Test Set** using F1-macro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtvovM1OhweD"
      },
      "source": [
        "6.1 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "0McWETfff-Eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_card = 'cardiffnlp/twitter-roberta-base-hate'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_card)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8NY4I6Hh1u_"
      },
      "source": [
        "6.2 Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "EhobuhaOh6ML"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_card, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "C_mLRA2FYEpu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCRZhZD9h9Mw"
      },
      "source": [
        "6.3 Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "8P6YZ9-hiCji"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2870/2870 [00:00<00:00, 17937.65 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [00:00<00:00, 10112.93 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286/286 [00:00<00:00, 18306.51 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "# Convert DataFrames to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(training_df[['tweet', 'hard_label_task1']])\n",
        "val_dataset = Dataset.from_pandas(validation_df[['tweet', 'hard_label_task1']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['tweet', 'hard_label_task1']])\n",
        "\n",
        "# Tokenize datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['tweet'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "c-p_msqtiLMG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['tweet', 'hard_label_task1', 'id_EXIST', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 2870\n",
            "})\n",
            "Dataset({\n",
            "    features: ['tweet', 'hard_label_task1', 'id_EXIST', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 286\n",
            "})\n",
            "Dataset({\n",
            "    features: ['tweet', 'hard_label_task1', 'id_EXIST', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 158\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_train)\n",
        "print(tokenized_test)\n",
        "print(tokenized_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "FFrSz27QiOUM"
      },
      "outputs": [],
      "source": [
        "# Prepare datasets for PyTorch\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"hard_label_task1\"])\n",
        "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"hard_label_task1\"])\n",
        "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"hard_label_task1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WcTpqpi9iWFE"
      },
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "train_features = {\n",
        "    'input_ids': tokenized_train['input_ids'],\n",
        "    'attention_mask': tokenized_train['attention_mask']\n",
        "}\n",
        "train_labels = tokenized_train['hard_label_task1']\n",
        "\n",
        "val_features = {\n",
        "    'input_ids': tokenized_val['input_ids'],\n",
        "    'attention_mask': tokenized_val['attention_mask']\n",
        "}\n",
        "val_labels = tokenized_val['hard_label_task1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFC5GSADiIO7"
      },
      "source": [
        "6.4 Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "KiUmRX5gidP-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def compute_metrics(output_info):\n",
        "    predictions, labels = output_info\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n",
        "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
        "    return {'f1': f1, 'acc': acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "TqgCp50nYzGH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\win11\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "LklOL5Nyif9I"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "acc_metric = evaluate.load('accuracy')\n",
        "f1_metric = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(output_info):\n",
        "    predictions, labels = output_info\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
        "    return {**f1, **acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AMRHCeSviioF"
      },
      "outputs": [],
      "source": [
        "tokenized_train = tokenized_train.rename_column(\"hard_label_task1\", \"labels\")\n",
        "tokenized_val = tokenized_val.rename_column(\"hard_label_task1\", \"labels\")\n",
        "tokenized_test = tokenized_test.rename_column(\"hard_label_task1\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "MbB540_yilS1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\WIN11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_3400\\4136018986.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,           # Keep only the best model\n",
        "    load_best_model_at_end=True,  # Load the best model at the end\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "xryARtcaio0k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|‚ñà‚ñà‚ñà‚ñé      | 180/540 [06:14<12:29,  2.08s/it]\n",
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2272, 'grad_norm': 12.275396347045898, 'learning_rate': 1.962962962962963e-05, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2847, 'grad_norm': 11.578030586242676, 'learning_rate': 1.925925925925926e-05, 'epoch': 0.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2364, 'grad_norm': 3.021670341491699, 'learning_rate': 1.888888888888889e-05, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2609, 'grad_norm': 9.262240409851074, 'learning_rate': 1.851851851851852e-05, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2061, 'grad_norm': 12.458492279052734, 'learning_rate': 1.814814814814815e-05, 'epoch': 0.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1809, 'grad_norm': 15.225414276123047, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2552, 'grad_norm': 5.072356700897217, 'learning_rate': 1.740740740740741e-05, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2495, 'grad_norm': 6.513915061950684, 'learning_rate': 1.7037037037037038e-05, 'epoch': 0.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                          \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2336, 'grad_norm': 9.231821060180664, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3556, 'grad_norm': 18.351810455322266, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3075, 'grad_norm': 8.211222648620605, 'learning_rate': 1.5925925925925926e-05, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3147, 'grad_norm': 6.408516883850098, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2785, 'grad_norm': 15.49801254272461, 'learning_rate': 1.5185185185185187e-05, 'epoch': 0.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2771, 'grad_norm': 15.821948051452637, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.78}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2511, 'grad_norm': 17.663267135620117, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2819, 'grad_norm': 17.289377212524414, 'learning_rate': 1.4074074074074075e-05, 'epoch': 0.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3894, 'grad_norm': 20.871492385864258, 'learning_rate': 1.3703703703703706e-05, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2155, 'grad_norm': 12.781991004943848, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2618\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2618\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2622\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:3049\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[0;32m   3047\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3049\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3050\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:3003\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3003\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3006\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:4050\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4047\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4049\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4050\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4051\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4053\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4060\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:4339\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4337\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4338\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4339\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4343\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
            "Cell \u001b[1;32mIn[65], line 10\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(output_info)\u001b[0m\n\u001b[0;32m      7\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m output_info\n\u001b[0;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mf1_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m acc \u001b[38;5;241m=\u001b[39m acc_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mf1, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39macc}\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\evaluate\\module.py:467\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[1;32m--> 467\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--f1\\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974\\f1.py:130\u001b[0m, in \u001b[0;36mF1._compute\u001b[1;34m(self, predictions, references, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute\u001b[39m(\u001b[38;5;28mself\u001b[39m, predictions, references, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    127\u001b[0m     score \u001b[38;5;241m=\u001b[39m f1_score(\n\u001b[0;32m    128\u001b[0m         references, predictions, labels\u001b[38;5;241m=\u001b[39mlabels, pos_label\u001b[38;5;241m=\u001b[39mpos_label, average\u001b[38;5;241m=\u001b[39maverage, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m    129\u001b[0m     )\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(score) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mscore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m score}\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'size'"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2870/2870 [00:00<00:00, 15697.70 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [00:00<00:00, 8275.06 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286/286 [00:00<00:00, 16978.10 examples/s]\n",
            "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_3400\\2107371415.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            "  2%|‚ñè         | 10/540 [00:27<15:58,  1.81s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.49, 'grad_norm': 12.456476211547852, 'learning_rate': 1.962962962962963e-05, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            "  4%|‚ñé         | 20/540 [00:44<14:34,  1.68s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4965, 'grad_norm': 8.99698543548584, 'learning_rate': 1.925925925925926e-05, 'epoch': 0.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            "  6%|‚ñå         | 30/540 [01:01<14:21,  1.69s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4367, 'grad_norm': 8.20038890838623, 'learning_rate': 1.888888888888889e-05, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            "  7%|‚ñã         | 40/540 [01:17<13:18,  1.60s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4004, 'grad_norm': 9.013848304748535, 'learning_rate': 1.851851851851852e-05, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            "  9%|‚ñâ         | 50/540 [01:33<13:29,  1.65s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3546, 'grad_norm': 5.8409552574157715, 'learning_rate': 1.814814814814815e-05, 'epoch': 0.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            " 11%|‚ñà         | 60/540 [01:49<12:55,  1.62s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3582, 'grad_norm': 14.881936073303223, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            " 13%|‚ñà‚ñé        | 70/540 [02:05<12:30,  1.60s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4086, 'grad_norm': 12.693936347961426, 'learning_rate': 1.740740740740741e-05, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            " 15%|‚ñà‚ñç        | 80/540 [02:21<12:10,  1.59s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3982, 'grad_norm': 7.906337738037109, 'learning_rate': 1.7037037037037038e-05, 'epoch': 0.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                \n",
            "\u001b[A                    \n",
            "\n",
            " 17%|‚ñà‚ñã        | 90/540 [02:37<11:54,  1.59s/it] \n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3623, 'grad_norm': 8.259791374206543, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 19%|‚ñà‚ñä        | 100/540 [02:53<11:37,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4451, 'grad_norm': 7.563941955566406, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 20%|‚ñà‚ñà        | 110/540 [03:09<11:20,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3991, 'grad_norm': 4.908424377441406, 'learning_rate': 1.5925925925925926e-05, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 22%|‚ñà‚ñà‚ñè       | 120/540 [03:24<11:06,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4018, 'grad_norm': 8.518524169921875, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 24%|‚ñà‚ñà‚ñç       | 130/540 [03:40<10:48,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4193, 'grad_norm': 14.791494369506836, 'learning_rate': 1.5185185185185187e-05, 'epoch': 0.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 26%|‚ñà‚ñà‚ñå       | 140/540 [03:56<10:34,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3637, 'grad_norm': 11.78187084197998, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.78}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 28%|‚ñà‚ñà‚ñä       | 150/540 [04:12<10:17,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3665, 'grad_norm': 7.2517242431640625, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 30%|‚ñà‚ñà‚ñâ       | 160/540 [04:28<10:02,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3497, 'grad_norm': 12.687145233154297, 'learning_rate': 1.4074074074074075e-05, 'epoch': 0.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 31%|‚ñà‚ñà‚ñà‚ñè      | 170/540 [04:44<09:46,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4561, 'grad_norm': 11.746819496154785, 'learning_rate': 1.3703703703703706e-05, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 33%|‚ñà‚ñà‚ñà‚ñé      | 180/540 [04:59<08:08,  1.36s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4405, 'grad_norm': 21.91253662109375, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\n",
            "\n",
            " 33%|‚ñà‚ñà‚ñà‚ñé      | 180/540 [05:03<08:08,  1.36s/it]\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3206433653831482, 'eval_f1': 0.8617672790901136, 'eval_accuracy': 0.8670886075949367, 'eval_runtime': 3.8514, 'eval_samples_per_second': 41.024, 'eval_steps_per_second': 2.596, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 35%|‚ñà‚ñà‚ñà‚ñå      | 190/540 [05:24<09:50,  1.69s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3061, 'grad_norm': 6.728715419769287, 'learning_rate': 1.2962962962962964e-05, 'epoch': 1.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 37%|‚ñà‚ñà‚ñà‚ñã      | 200/540 [05:40<09:00,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3255, 'grad_norm': 8.545354843139648, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 39%|‚ñà‚ñà‚ñà‚ñâ      | 210/540 [05:56<08:43,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.233, 'grad_norm': 13.446605682373047, 'learning_rate': 1.2222222222222224e-05, 'epoch': 1.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 41%|‚ñà‚ñà‚ñà‚ñà      | 220/540 [06:11<08:27,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2331, 'grad_norm': 1.597847819328308, 'learning_rate': 1.1851851851851852e-05, 'epoch': 1.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 230/540 [06:27<08:14,  1.60s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1786, 'grad_norm': 1.8005441427230835, 'learning_rate': 1.1481481481481482e-05, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 240/540 [06:43<07:56,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3071, 'grad_norm': 12.097139358520508, 'learning_rate': 1.1111111111111113e-05, 'epoch': 1.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 250/540 [06:59<07:39,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.228, 'grad_norm': 10.384810447692871, 'learning_rate': 1.0740740740740742e-05, 'epoch': 1.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 260/540 [07:15<07:22,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2339, 'grad_norm': 2.9036543369293213, 'learning_rate': 1.037037037037037e-05, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 270/540 [07:31<07:08,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.187, 'grad_norm': 12.73487377166748, 'learning_rate': 1e-05, 'epoch': 1.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 280/540 [07:47<06:53,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1731, 'grad_norm': 23.254257202148438, 'learning_rate': 9.62962962962963e-06, 'epoch': 1.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 290/540 [08:03<06:36,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2078, 'grad_norm': 4.181375026702881, 'learning_rate': 9.25925925925926e-06, 'epoch': 1.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 300/540 [08:19<06:21,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4344, 'grad_norm': 13.325477600097656, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 310/540 [08:35<06:06,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1682, 'grad_norm': 11.66923999786377, 'learning_rate': 8.518518518518519e-06, 'epoch': 1.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 320/540 [08:51<05:48,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1943, 'grad_norm': 14.338790893554688, 'learning_rate': 8.148148148148148e-06, 'epoch': 1.78}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 330/540 [09:06<05:33,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2923, 'grad_norm': 6.884762763977051, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 340/540 [09:22<05:17,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3255, 'grad_norm': 9.990431785583496, 'learning_rate': 7.4074074074074075e-06, 'epoch': 1.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 350/540 [09:38<05:00,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.284, 'grad_norm': 14.00877571105957, 'learning_rate': 7.0370370370370375e-06, 'epoch': 1.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 360/540 [09:53<04:02,  1.35s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2553, 'grad_norm': 11.612592697143555, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\n",
            "\n",
            " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 360/540 [09:57<04:02,  1.35s/it]\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3241322636604309, 'eval_f1': 0.8630905714875181, 'eval_accuracy': 0.8670886075949367, 'eval_runtime': 3.7661, 'eval_samples_per_second': 41.954, 'eval_steps_per_second': 2.655, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 370/540 [10:16<04:42,  1.66s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.118, 'grad_norm': 5.444835186004639, 'learning_rate': 6.296296296296297e-06, 'epoch': 2.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 380/540 [10:32<04:14,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1828, 'grad_norm': 17.1771240234375, 'learning_rate': 5.925925925925926e-06, 'epoch': 2.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 390/540 [10:48<03:59,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1307, 'grad_norm': 5.268555164337158, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 400/540 [11:04<03:41,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2094, 'grad_norm': 12.586209297180176, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 410/540 [11:20<03:28,  1.61s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1018, 'grad_norm': 11.985681533813477, 'learning_rate': 4.814814814814815e-06, 'epoch': 2.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 420/540 [11:36<03:11,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2238, 'grad_norm': 30.513235092163086, 'learning_rate': 4.444444444444444e-06, 'epoch': 2.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 430/540 [11:52<02:54,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1392, 'grad_norm': 57.21003341674805, 'learning_rate': 4.074074074074074e-06, 'epoch': 2.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 440/540 [12:08<02:38,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1745, 'grad_norm': 17.964122772216797, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 450/540 [12:24<02:22,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1808, 'grad_norm': 27.978553771972656, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 460/540 [12:40<02:06,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1219, 'grad_norm': 7.007600784301758, 'learning_rate': 2.962962962962963e-06, 'epoch': 2.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 470/540 [12:55<01:50,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1778, 'grad_norm': 11.6115140914917, 'learning_rate': 2.5925925925925925e-06, 'epoch': 2.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 480/540 [13:11<01:34,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.105, 'grad_norm': 2.12923002243042, 'learning_rate': 2.222222222222222e-06, 'epoch': 2.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 490/540 [13:27<01:19,  1.58s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1874, 'grad_norm': 5.670341968536377, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 500/540 [13:43<01:03,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2115, 'grad_norm': 6.901687145233154, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 510/540 [13:59<00:47,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1088, 'grad_norm': 15.399652481079102, 'learning_rate': 1.111111111111111e-06, 'epoch': 2.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 520/540 [14:15<00:31,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1805, 'grad_norm': 2.6565890312194824, 'learning_rate': 7.407407407407407e-07, 'epoch': 2.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 530/540 [14:31<00:15,  1.59s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1768, 'grad_norm': 18.63205337524414, 'learning_rate': 3.7037037037037036e-07, 'epoch': 2.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 540/540 [14:46<00:00,  1.35s/it]\n",
            "\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0703, 'grad_norm': 10.994009017944336, 'learning_rate': 0.0, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            "\u001b[A\u001b[A                                           \n",
            "\n",
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 540/540 [14:53<00:00,  1.35s/it]\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4401385188102722, 'eval_f1': 0.8727696545878364, 'eval_accuracy': 0.879746835443038, 'eval_runtime': 3.8502, 'eval_samples_per_second': 41.037, 'eval_steps_per_second': 2.597, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \n",
            "\u001b[A                    \n",
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 540/540 [14:57<00:00,  1.35s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 540/540 [14:57<00:00,  1.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 898.0766, 'train_samples_per_second': 9.587, 'train_steps_per_second': 0.601, 'train_loss': 0.27251249684227835, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=540, training_loss=0.27251249684227835, metrics={'train_runtime': 898.0766, 'train_samples_per_second': 9.587, 'train_steps_per_second': 0.601, 'total_flos': 566346546662400.0, 'train_loss': 0.27251249684227835, 'epoch': 3.0})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import evaluate\n",
        "\n",
        "# Model and Tokenizer\n",
        "model_card = 'cardiffnlp/twitter-roberta-base-hate'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_card, num_labels=2)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(training_df[['tweet', 'hard_label_task1']])\n",
        "val_dataset = Dataset.from_pandas(validation_df[['tweet', 'hard_label_task1']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['tweet', 'hard_label_task1']])\n",
        "\n",
        "# Tokenize datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['tweet'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Rename the label column for Trainer compatibility\n",
        "tokenized_train = tokenized_train.rename_column(\"hard_label_task1\", \"labels\")\n",
        "tokenized_val = tokenized_val.rename_column(\"hard_label_task1\", \"labels\")\n",
        "tokenized_test = tokenized_test.rename_column(\"hard_label_task1\", \"labels\")\n",
        "\n",
        "# Format datasets for PyTorch\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Define evaluation metrics\n",
        "def compute_metrics(output_info):\n",
        "    predictions, labels = output_info.predictions, output_info.label_ids\n",
        "    predictions = np.argmax(predictions, axis=-1)  # Convert logits to class predictions\n",
        "    \n",
        "    f1 = f1_score(y_true=labels, y_pred=predictions, average='macro')  # 'macro' for multi-class\n",
        "    acc = accuracy_score(y_true=labels, y_pred=predictions)\n",
        "    return {'f1': f1, 'accuracy': acc}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VjkPI6itfz"
      },
      "source": [
        "6.5 Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "J46vhiQAirxP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:06<00:00,  2.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results:\n",
            "{'eval_loss': 0.4742465913295746, 'eval_f1': 0.8263595474989778, 'eval_accuracy': 0.8286713286713286, 'eval_runtime': 6.9151, 'eval_samples_per_second': 41.359, 'eval_steps_per_second': 2.603, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the test set\n",
        "test_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "\n",
        "print(\"Test Results:\")\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gtiG2mAL3HM"
      },
      "source": [
        "# [Task 7 - 0.5 points] Error Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "After evaluating the model, perform a brief error analysis:\n",
        "\n",
        " - Review the results and identify common errors.\n",
        "\n",
        " - Summarize your findings regarding the errors and their impact on performance (e.g. but not limited to Out-of-Vocabulary (OOV) words, data imbalance, and performance differences between the custom model and the transformer...)\n",
        " - Suggest possible solutions to address the identified errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "i_9uVGUcRWKd"
      },
      "outputs": [],
      "source": [
        "#steps for error analysis\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "qrNPEb6FRcC-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary of Results:\n",
            "Baseline Model - F1-Scores across seeds: [0.804547199590496, 0.7510504201680672, 0.7656283110828566]\n",
            "Model 1 - F1-Scores across seeds: [0.7656283110828566, 0.7656283110828566, 0.7656283110828566]\n",
            "Baseline Model - Avg Macro F1-Score across seeds: 0.7737\n",
            "Model 1 - Avg Macro F1-Score across seeds: 0.7656\n"
          ]
        }
      ],
      "source": [
        "# Summarize Results\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(\"Baseline Model - F1-Scores across seeds:\", baseline_f1_scores)\n",
        "print(\"Model 1 - F1-Scores across seeds:\", model_1_f1_scores)\n",
        "print(f\"Baseline Model - Avg Macro F1-Score across seeds: {np.mean(baseline_f1_scores):.4f}\")\n",
        "print(f\"Model 1 - Avg Macro F1-Score across seeds: {np.mean(model_1_f1_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcIfdqswRgXE"
      },
      "source": [
        "The noticeable thing here is that Model 1 is much more stable across seeds, and therefore the model is not affected by randomness. This means that the model is extremely robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "8ZBKimDYRfc-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Not Sexist       0.79      0.93      0.85        90\n",
            "      Sexist       0.88      0.66      0.76        68\n",
            "\n",
            "    accuracy                           0.82       158\n",
            "   macro avg       0.83      0.80      0.80       158\n",
            "weighted avg       0.83      0.82      0.81       158\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(best_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "5BdSTgPkSJfy"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_val (true labels) and final_val_predictions (predicted labels) are defined\n",
        "cm = confusion_matrix(y_val, final_val_predictions)\n",
        "\n",
        "# Create a ConfusionMatrixDisplay object\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Sexist', 'Sexist'])  # Update labels as per your dataset\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot(cmap=\"Blues\")  # Choose a colormap you prefer, e.g., \"Blues\"\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8LOJXv2TaLx"
      },
      "source": [
        "We see that in general it seems more difficult for the model to recognize sexist tweets, as there tends to be more mistakes in that department."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "wY5HiuWFSNgS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNK Token Statistics by Model Prediction:\n",
            "            total_tweets  total_unk  avg_unk_per_tweet  avg_unk_percentage\n",
            "Not Sexist           107        367           3.429907            0.124423\n",
            "Sexist                51        126           2.470588            0.088399\n"
          ]
        }
      ],
      "source": [
        "# Function to count `[UNK]` tokens\n",
        "def count_unk_tokens(tweet, vocabulary):\n",
        "    return sum(1 for token in tweet.split() if token.lower() not in vocabulary)\n",
        "\n",
        "# Add `[UNK]` counts to the validation set\n",
        "validation_df['unk_count'] = validation_df['tweet'].apply(lambda tweet: count_unk_tokens(tweet, vocabulary))\n",
        "validation_df['unk_percentage'] = validation_df['unk_count'] / validation_df['tweet'].apply(lambda tweet: len(tweet.split()))\n",
        "\n",
        "# Add model predictions\n",
        "validation_df['predicted_label'] = final_val_predictions\n",
        "\n",
        "# Group by prediction and calculate `[UNK]` statistics\n",
        "unk_by_prediction = validation_df.groupby('predicted_label').agg(\n",
        "    total_tweets=('tweet', 'count'),\n",
        "    total_unk=('unk_count', 'sum'),\n",
        "    avg_unk_per_tweet=('unk_count', 'mean'),\n",
        "    avg_unk_percentage=('unk_percentage', 'mean')\n",
        ")\n",
        "\n",
        "# Map numeric labels to class names\n",
        "unk_by_prediction.index = ['Not Sexist', 'Sexist']\n",
        "\n",
        "# Print results\n",
        "print(\"UNK Token Statistics by Model Prediction:\")\n",
        "print(unk_by_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0wZHWUsTDmd"
      },
      "source": [
        "This code underlines the impact of the [UNK] label in the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-0PZDTyTr_t"
      },
      "source": [
        "Furtermore, we noticed that the model would benefit from data augmentation, which cannot be done in this case as it would be too complicated and not accurate, but in general having more data, artificial or real would be good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P42XYjb6K3k5"
      },
      "source": [
        "# [Task 8 - 0.5 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9oXSaW1K5S7"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHw2L6PlLDyE"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUqh1utLflM"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypagJed7LheY"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgtFwKXMLjww"
      },
      "source": [
        "### Execution Order\n",
        "\n",
        "You are **free** to address tasks in any order (if multiple orderings are available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BjMk5e_M4n7"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TVgpYlM6s5"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "However, you are **free** to play with their hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia6IapI1M_A7"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDaW8DyNBu5"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1WcrpemNEQm"
      },
      "source": [
        "### Robust Evaluation\n",
        "\n",
        "Each model is trained with at least 3 random seeds.\n",
        "\n",
        "Task 4 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mVe5dqzNI_u"
      },
      "source": [
        "### Model Selection for Analysis\n",
        "\n",
        "To carry out the error analysis you are **free** to either\n",
        "\n",
        "* Pick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\n",
        "* Perform ensembling via, for instance, majority voting to obtain a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8a4pDKSNKzI"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OJm14UZkThSt"
      },
      "source": [
        "### Bonus Points\n",
        "Bonus points are arbitrarily assigned based on significant contributions such as:\n",
        "- Outstanding error analysis\n",
        "- Masterclass code organization\n",
        "- Suitable extensions\n",
        "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6).\n",
        "\n",
        "**Possible Extensions/Explorations for Bonus Points:**\n",
        "- **Try other preprocessing strategies**: e.g., but not limited to, explore techniques tailored specifically for tweets or  methods that are common in social media text.\n",
        "- **Experiment with other custom architectures or models from HuggingFace**\n",
        "- **Explore Spanish tweets**: e.g., but not limited to, leverage multilingual models to process Spanish tweets and assess their performance compared to monolingual models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmMKE7vLu-y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# The End"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
